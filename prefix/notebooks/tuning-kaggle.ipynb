{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a343064-c5ae-45ea-8ea9-a7af82b483c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ab414f5-40a7-482e-9cfc-b85508b93ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['hf_token'] = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "os.environ['WANDB_API_KEY'] = user_secrets.get_secret(\"WANDB_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "510f901c-c529-4b59-897a-5eb9186f5122",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(os.getenv('hf_token'))\n",
    "assert(os.getenv('WANDB_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d645e005-3a1a-46f5-865f-4fd0894a330b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (24.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.5.1+cu124)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.51.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.3.0)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.30.2)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.3)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.21.0)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.1)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (2025.3.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (2.4.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13.0->peft)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13.0->peft)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13.0->peft)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13.0->peft)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13.0->peft)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13.0->peft)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.21.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->peft) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->peft) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->peft) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->peft) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->peft) (2024.2.0)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.9.90\n",
      "    Uninstalling nvidia-curand-cu12-10.3.9.90:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install peft wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fe9f6a4-1435-484c-8a99-080c9746b57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 07:47:18.729168: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746517638.923248      71 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746517638.979782      71 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from peft import PrefixTuningConfig, get_peft_model\n",
    "import wandb\n",
    "from huggingface_hub import login\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "529a4751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge_score) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge_score) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rouge_score) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rouge_score) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rouge_score) (2024.2.0)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=3582bdadb6f6c5e4d17d643e4c79e3ece64d314587e9d7b98f7409f86279c549\n",
      "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: rouge_score\n",
      "Successfully installed rouge_score-0.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c9f8375-f538-49a3-8b3c-db8f7c836919",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91a96fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025/May/06 - 13:17:39\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "now_utc = datetime.now(pytz.utc)\n",
    "now_colombo = now_utc.astimezone(pytz.timezone('Asia/Colombo'))\n",
    "run_name = now_colombo.strftime('%Y/%b/%d - %H:%M:%S')\n",
    "print(run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d48c9fc9-1d56-4811-9c43-73b1186c8334",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrtweera\u001b[0m (\u001b[33mrtw-rtweera\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250506_074745-ix4rcs7s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rtw-rtweera/prefix-tuning-qwen-25-question-generation/runs/ix4rcs7s' target=\"_blank\">2025/May/06 - 13:17:39</a></strong> to <a href='https://wandb.ai/rtw-rtweera/prefix-tuning-qwen-25-question-generation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rtw-rtweera/prefix-tuning-qwen-25-question-generation' target=\"_blank\">https://wandb.ai/rtw-rtweera/prefix-tuning-qwen-25-question-generation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rtw-rtweera/prefix-tuning-qwen-25-question-generation/runs/ix4rcs7s' target=\"_blank\">https://wandb.ai/rtw-rtweera/prefix-tuning-qwen-25-question-generation/runs/ix4rcs7s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/rtw-rtweera/prefix-tuning-qwen-25-question-generation/runs/ix4rcs7s?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7e2d6d68fe10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=os.getenv('WANDB_API_KEY'))\n",
    "wandb.init(project=\"prefix-tuning-qwen-25-question-generation\", name=run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04580276-5c8e-493e-b308-db66fbbd856b",
   "metadata": {},
   "outputs": [],
   "source": [
    "login(token=os.getenv('hf_token'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56027f61-695c-46c1-aa90-e7819bcb3560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f581ca58367e48d2960685fb8dffcedd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2f0cc555d544553867c9fcc01360a76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ed21e5d50ad430cb1177531e8b51f17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/1.82M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f155ba9465d4685bdf51da9e08588e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d01d57f42bf4161b6d7d998c0f2010b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b27ca873-e83a-4732-88ea-f87e3dfe2588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b09b792-e644-4a2a-af97-7950cd6fad13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.dataset_dict.DatasetDict"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "428c4cdc-99a7-4216-8df4-95f38115e638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a33e438-0725-443a-bb4b-e3879c521ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f55749f-a71a-4a55-8a83-2b2038b80bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answers': {'answer_start': [74],\n",
      "             'text': ['Michigan Wolverines football team']},\n",
      " 'context': 'The Notre Dame football team has a long history, first beginning '\n",
      "            'when the Michigan Wolverines football team brought football to '\n",
      "            'Notre Dame in 1887 and played against a group of students. In the '\n",
      "            'long history since then, 13 Fighting Irish teams have won '\n",
      "            'consensus national championships (although the university only '\n",
      "            'claims 11), along with another nine teams being named national '\n",
      "            'champion by at least one source. Additionally, the program has '\n",
      "            'the most members in the College Football Hall of Fame, is tied '\n",
      "            'with Ohio State University with the most Heisman Trophies won, '\n",
      "            'and have the highest winning percentage in NCAA history. With the '\n",
      "            'long history, Notre Dame has accumulated many rivals, and its '\n",
      "            'annual game against USC for the Jeweled Shillelagh has been named '\n",
      "            'by some as one of the most important in college football and is '\n",
      "            'often called the greatest intersectional rivalry in college '\n",
      "            'football in the country.',\n",
      " 'id': '5733c4494776f419006611da',\n",
      " 'question': \"Which team did Notre Dame's football team find inspiration from?\",\n",
      " 'title': 'University_of_Notre_Dame'}\n"
     ]
    }
   ],
   "source": [
    "pprint(dataset['train'][234])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20871c3d-2154-428f-a33d-e9b2c547f5b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5faf237c899c4d109aac481657a304f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.23k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0c073491d17412d962bee9ccf132979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68caaf0f64a746adbb67e18c6b95e0ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb67283fbd1e47538177bbc0b730b9ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'Qwen/Qwen2.5-0.5B'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f50737fc-218c-4e65-87d4-ee8b4cd22d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01aa6c80-9b91-4792-8b6c-270f0622eeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(example_batch):\n",
    "    prompt = 'Generate a question from this given context:'\n",
    "    inputs = [f'{prompt} {context} -> {question}' for context, question in zip(example_batch['context'], example_batch['question'])]\n",
    "    tokenized = tokenizer(\n",
    "        inputs,\n",
    "        max_length=256,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    tokenized['labels'] = tokenized['input_ids'].clone()\n",
    "    return tokenized\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f91c7e8a-219c-4568-80bf-9191534292b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d22082595baa48e58654576b4f0e13a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef9360fc3f004b5e94951055f86fe4da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized = dataset.map(preprocess, batched=True, remove_columns = dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6babe391-c42a-4c33-843f-5f1b1e35642d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "670db571-673c-4f68-a52c-c48ddd5ed9dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [31115,\n",
       "  264,\n",
       "  3405,\n",
       "  504,\n",
       "  419,\n",
       "  2661,\n",
       "  2266,\n",
       "  25,\n",
       "  7297,\n",
       "  20288,\n",
       "  220,\n",
       "  20,\n",
       "  15,\n",
       "  572,\n",
       "  458,\n",
       "  3693,\n",
       "  8964,\n",
       "  1809,\n",
       "  311,\n",
       "  8253,\n",
       "  279,\n",
       "  18319,\n",
       "  315,\n",
       "  279,\n",
       "  5055,\n",
       "  20761,\n",
       "  8953,\n",
       "  320,\n",
       "  87101,\n",
       "  8,\n",
       "  369,\n",
       "  279,\n",
       "  220,\n",
       "  17,\n",
       "  15,\n",
       "  16,\n",
       "  20,\n",
       "  3200,\n",
       "  13,\n",
       "  576,\n",
       "  3693,\n",
       "  20761,\n",
       "  14872,\n",
       "  320,\n",
       "  32,\n",
       "  6754,\n",
       "  8,\n",
       "  18319,\n",
       "  22117,\n",
       "  41594,\n",
       "  23283,\n",
       "  279,\n",
       "  5055,\n",
       "  20761,\n",
       "  14872,\n",
       "  320,\n",
       "  45,\n",
       "  6754,\n",
       "  8,\n",
       "  18319,\n",
       "  12740,\n",
       "  44167,\n",
       "  220,\n",
       "  17,\n",
       "  19,\n",
       "  4142,\n",
       "  16,\n",
       "  15,\n",
       "  311,\n",
       "  7232,\n",
       "  862,\n",
       "  4843,\n",
       "  7297,\n",
       "  20288,\n",
       "  2265,\n",
       "  13,\n",
       "  576,\n",
       "  1809,\n",
       "  572,\n",
       "  6342,\n",
       "  389,\n",
       "  7400,\n",
       "  220,\n",
       "  22,\n",
       "  11,\n",
       "  220,\n",
       "  17,\n",
       "  15,\n",
       "  16,\n",
       "  21,\n",
       "  11,\n",
       "  518,\n",
       "  55041,\n",
       "  594,\n",
       "  22636,\n",
       "  304,\n",
       "  279,\n",
       "  5836,\n",
       "  12879,\n",
       "  9154,\n",
       "  12030,\n",
       "  518,\n",
       "  15993,\n",
       "  50557,\n",
       "  11,\n",
       "  7043,\n",
       "  13,\n",
       "  1634,\n",
       "  419,\n",
       "  572,\n",
       "  279,\n",
       "  220,\n",
       "  20,\n",
       "  15,\n",
       "  339,\n",
       "  7297,\n",
       "  20288,\n",
       "  11,\n",
       "  279,\n",
       "  10734,\n",
       "  45628,\n",
       "  279,\n",
       "  330,\n",
       "  97235,\n",
       "  21582,\n",
       "  1,\n",
       "  448,\n",
       "  5257,\n",
       "  6623,\n",
       "  56589,\n",
       "  27172,\n",
       "  11,\n",
       "  438,\n",
       "  1632,\n",
       "  438,\n",
       "  27092,\n",
       "  9298,\n",
       "  2459,\n",
       "  279,\n",
       "  13815,\n",
       "  315,\n",
       "  34948,\n",
       "  1817,\n",
       "  7297,\n",
       "  20288,\n",
       "  1809,\n",
       "  448,\n",
       "  12751,\n",
       "  7857,\n",
       "  1127,\n",
       "  320,\n",
       "  7995,\n",
       "  892,\n",
       "  279,\n",
       "  1809,\n",
       "  1035,\n",
       "  614,\n",
       "  1012,\n",
       "  3881,\n",
       "  438,\n",
       "  330,\n",
       "  19284,\n",
       "  20288,\n",
       "  444,\n",
       "  3975,\n",
       "  773,\n",
       "  429,\n",
       "  279,\n",
       "  12426,\n",
       "  1410,\n",
       "  72988,\n",
       "  4565,\n",
       "  279,\n",
       "  34117,\n",
       "  7857,\n",
       "  1127,\n",
       "  220,\n",
       "  20,\n",
       "  15,\n",
       "  13,\n",
       "  1464,\n",
       "  15920,\n",
       "  12588,\n",
       "  2083,\n",
       "  15251,\n",
       "  279,\n",
       "  63536,\n",
       "  518,\n",
       "  7297,\n",
       "  20288,\n",
       "  220,\n",
       "  20,\n",
       "  15,\n",
       "  30,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'labels': [31115,\n",
       "  264,\n",
       "  3405,\n",
       "  504,\n",
       "  419,\n",
       "  2661,\n",
       "  2266,\n",
       "  25,\n",
       "  7297,\n",
       "  20288,\n",
       "  220,\n",
       "  20,\n",
       "  15,\n",
       "  572,\n",
       "  458,\n",
       "  3693,\n",
       "  8964,\n",
       "  1809,\n",
       "  311,\n",
       "  8253,\n",
       "  279,\n",
       "  18319,\n",
       "  315,\n",
       "  279,\n",
       "  5055,\n",
       "  20761,\n",
       "  8953,\n",
       "  320,\n",
       "  87101,\n",
       "  8,\n",
       "  369,\n",
       "  279,\n",
       "  220,\n",
       "  17,\n",
       "  15,\n",
       "  16,\n",
       "  20,\n",
       "  3200,\n",
       "  13,\n",
       "  576,\n",
       "  3693,\n",
       "  20761,\n",
       "  14872,\n",
       "  320,\n",
       "  32,\n",
       "  6754,\n",
       "  8,\n",
       "  18319,\n",
       "  22117,\n",
       "  41594,\n",
       "  23283,\n",
       "  279,\n",
       "  5055,\n",
       "  20761,\n",
       "  14872,\n",
       "  320,\n",
       "  45,\n",
       "  6754,\n",
       "  8,\n",
       "  18319,\n",
       "  12740,\n",
       "  44167,\n",
       "  220,\n",
       "  17,\n",
       "  19,\n",
       "  4142,\n",
       "  16,\n",
       "  15,\n",
       "  311,\n",
       "  7232,\n",
       "  862,\n",
       "  4843,\n",
       "  7297,\n",
       "  20288,\n",
       "  2265,\n",
       "  13,\n",
       "  576,\n",
       "  1809,\n",
       "  572,\n",
       "  6342,\n",
       "  389,\n",
       "  7400,\n",
       "  220,\n",
       "  22,\n",
       "  11,\n",
       "  220,\n",
       "  17,\n",
       "  15,\n",
       "  16,\n",
       "  21,\n",
       "  11,\n",
       "  518,\n",
       "  55041,\n",
       "  594,\n",
       "  22636,\n",
       "  304,\n",
       "  279,\n",
       "  5836,\n",
       "  12879,\n",
       "  9154,\n",
       "  12030,\n",
       "  518,\n",
       "  15993,\n",
       "  50557,\n",
       "  11,\n",
       "  7043,\n",
       "  13,\n",
       "  1634,\n",
       "  419,\n",
       "  572,\n",
       "  279,\n",
       "  220,\n",
       "  20,\n",
       "  15,\n",
       "  339,\n",
       "  7297,\n",
       "  20288,\n",
       "  11,\n",
       "  279,\n",
       "  10734,\n",
       "  45628,\n",
       "  279,\n",
       "  330,\n",
       "  97235,\n",
       "  21582,\n",
       "  1,\n",
       "  448,\n",
       "  5257,\n",
       "  6623,\n",
       "  56589,\n",
       "  27172,\n",
       "  11,\n",
       "  438,\n",
       "  1632,\n",
       "  438,\n",
       "  27092,\n",
       "  9298,\n",
       "  2459,\n",
       "  279,\n",
       "  13815,\n",
       "  315,\n",
       "  34948,\n",
       "  1817,\n",
       "  7297,\n",
       "  20288,\n",
       "  1809,\n",
       "  448,\n",
       "  12751,\n",
       "  7857,\n",
       "  1127,\n",
       "  320,\n",
       "  7995,\n",
       "  892,\n",
       "  279,\n",
       "  1809,\n",
       "  1035,\n",
       "  614,\n",
       "  1012,\n",
       "  3881,\n",
       "  438,\n",
       "  330,\n",
       "  19284,\n",
       "  20288,\n",
       "  444,\n",
       "  3975,\n",
       "  773,\n",
       "  429,\n",
       "  279,\n",
       "  12426,\n",
       "  1410,\n",
       "  72988,\n",
       "  4565,\n",
       "  279,\n",
       "  34117,\n",
       "  7857,\n",
       "  1127,\n",
       "  220,\n",
       "  20,\n",
       "  15,\n",
       "  13,\n",
       "  1464,\n",
       "  15920,\n",
       "  12588,\n",
       "  2083,\n",
       "  15251,\n",
       "  279,\n",
       "  63536,\n",
       "  518,\n",
       "  7297,\n",
       "  20288,\n",
       "  220,\n",
       "  20,\n",
       "  15,\n",
       "  30,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized['validation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7073fa25-4c46-4ecc-b9d3-76d22983baf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = tokenized['train'].select(range(1000)) \n",
    "eval_set = tokenized['validation'].select(range(40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac51bcdb-daca-4eba-b437-4dc0911187ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e467946887c842038f97ebbfceb90301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/681 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72933a357e964588aec8a9d0e5dbb360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "196a00a657c5435ca78856461d99670d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/138 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6da19b56-f055-45bc-ba9a-2280422c09d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate questions\n",
    "def generate_question(context, model, tokenizer, max_new_tokens=30):\n",
    "    input_text = f\"Generate a question: {context} -> \"\n",
    "    inputs = tokenizer(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=256,\n",
    "        truncation=True\n",
    "    ).to(model.device)\n",
    "    question_ids = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        num_beams=4,\n",
    "        early_stopping=True,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    question = tokenizer.decode(question_ids[0], skip_special_tokens=True)\n",
    "    # Extract question after \"->\"\n",
    "    question = question.split(\"->\")[-1].strip()\n",
    "    return question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ea2b18f-f7ff-47f1-abea-5e974512a637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate base model\n",
    "scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "base_bleu_scores = []\n",
    "base_rouge_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fddee684-a89c-4dea-a748-ca761da20ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten validation dataset for evaluation\n",
    "eval_contexts = []\n",
    "eval_questions = []\n",
    "for example in dataset[\"validation\"]:\n",
    "    eval_contexts.append(example['context'])\n",
    "    eval_questions.append(example['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8bee7b77-f733-4848-ae1b-aacac7b8c49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e66c7ea-58b2-41c8-a940-6e222ce03607",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 30/40 [00:29<00:09,  1.04it/s]/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "100%|██████████| 40/40 [00:38<00:00,  1.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on subset\n",
    "for i in tqdm(range(len(eval_set))):  # Match small_eval size\n",
    "    context = eval_contexts[i]\n",
    "    reference = eval_questions[i]\n",
    "    question = generate_question(context, base_model, tokenizer)\n",
    "    # BLEU score\n",
    "    bleu_score = sentence_bleu(reference.split(), question.split())\n",
    "    base_bleu_scores.append(bleu_score)\n",
    "    # ROUGE-L score\n",
    "    rouge_score = scorer.score(reference, question)[\"rougeL\"].fmeasure\n",
    "    base_rouge_scores.append(rouge_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0764a407-9dc1-4a01-bba1-d069eef67a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model Scores: {'bleu': 1.8717975852290112e-232, 'rougeL': 0.1343752489641756}\n"
     ]
    }
   ],
   "source": [
    "# Compute average scores\n",
    "base_bleu_avg = np.mean(base_bleu_scores)\n",
    "base_rouge_avg = np.mean(base_rouge_scores)\n",
    "print(\"Base Model Scores:\", {\"bleu\": base_bleu_avg, \"rougeL\": base_rouge_avg})\n",
    "\n",
    "# Log to W&B\n",
    "wandb.log({\"base_bleu\": base_bleu_avg, \"base_rougeL\": base_rouge_avg})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90374459-d7e8-4180-bbd9-bcdd35d5ee14",
   "metadata": {},
   "source": [
    "### Tuning part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bb81bc-fbb4-4067-891f-03577044d170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Configure Prefix Tuning\n",
    "# Define prefix tuning configuration\n",
    "peft_config = PrefixTuningConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    num_virtual_tokens=20,  # with 20, you hit upto 17GB of RAM usage - apparently it is due to the savings of logits, oversome with predict_with_generate = True\n",
    "    prefix_projection=True  # pass through an additional 2 layer MLP before attaching them to Key or Value - this enhances the expressiveness - hence more accurate. But more complex, risk of overfitting, more computational resources need, more time to train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ebf3da2a-21d3-4204-bcb7-8466b97d0838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 811,648 || all params: 494,844,416 || trainable%: 0.1640\n"
     ]
    }
   ],
   "source": [
    "# Load model and apply prefix tuning\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Move to GPU if available\n",
    "model = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a9697ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151643"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a5427942-1bd3-4525-8642-f96745158b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Define Metrics and Training Arguments\n",
    "# Compute BLEU and ROUGE metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # print('predictions type: ', type(predictions), 'shape: ', getattr(predictions, 'shape', 'N/A'))\n",
    "    # print('labels type: ', type(labels), 'shape: ', getattr(labels, 'shape', 'N/A'))\n",
    "    # O/P for above\n",
    "    # predictions type: <class 'numpy.ndarray'> shape: (20, 256, 151936)\n",
    "    # labels type: <class 'numpy.ndarray'> shape: (20, 256)\n",
    "\n",
    "    # in classification we get a 2D predictions tensor (batch size, # of classes) e.g. [[0.1, 0.5], [-0.6, 1.5]] = two batches, each with two classes and logit values for each class as int\n",
    "    # in LM, we get 3D predictions tensor (batch size, seq length, vocab size) e.g. [{(0.1, 0.5, ...to vocab size), (-0.6, 1.5, ...to vocab size), ...all sequences}, {next item in batch}]\n",
    "\n",
    "    predictions = predictions.argmax(-1)    # argmax of last axis (i.e. logits axis)\n",
    "    labels = np.where(labels == -100, tokenizer.pad_token_id, labels)   # labels where it should be ignored in loss (e.g. padding) are set to -100, which the tokenizer can't decode\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # Extract question part after \"->\"\n",
    "    decoded_preds = [pred.split(\"->\")[-1].strip() if \"->\" in pred else pred for pred in decoded_preds]\n",
    "    decoded_labels = [label.split(\"->\")[-1].strip() if \"->\" in label else label for label in decoded_labels]\n",
    "    scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "    bleu_scores = []\n",
    "    rouge_scores = []\n",
    "    for pred, ref in zip(decoded_preds, decoded_labels):\n",
    "        bleu_scores.append(sentence_bleu([ref.split()], pred.split()))\n",
    "        rouge_scores.append(scorer.score(ref, pred)[\"rougeL\"].fmeasure)\n",
    "    return {\n",
    "        \"bleu\": np.mean(bleu_scores),\n",
    "        \"rougeL\": np.mean(rouge_scores)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e974f0f-649a-4846-9fdb-2a4ee2b74a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen25-0.5b-prefix-tuning\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    eval_accumulation_steps=1,\n",
    "    num_train_epochs=8,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    logging_first_step=True,\n",
    "    eval_on_start=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=False,   # dont set this for PEFT methods, causees errors, manually load the best model\n",
    "    push_to_hub=True,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=f'{run_name}-prefix-tuned',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "55ac6907-7a94-4da7-8daf-199e0e5e8890",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "da9f349c-c489-4a87-b2fb-d21effd03a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=eval_set,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "78a72cfb-e306-4e4e-8d59-29a661c8376b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 16:20, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rougel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.682300</td>\n",
       "      <td>2.563648</td>\n",
       "      <td>0.001705</td>\n",
       "      <td>0.070086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.562900</td>\n",
       "      <td>2.386212</td>\n",
       "      <td>0.002233</td>\n",
       "      <td>0.071315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.401200</td>\n",
       "      <td>2.293324</td>\n",
       "      <td>0.002462</td>\n",
       "      <td>0.073889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.467600</td>\n",
       "      <td>2.232627</td>\n",
       "      <td>0.003662</td>\n",
       "      <td>0.072380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.412800</td>\n",
       "      <td>2.195016</td>\n",
       "      <td>0.004479</td>\n",
       "      <td>0.072105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.469000</td>\n",
       "      <td>2.171984</td>\n",
       "      <td>0.004458</td>\n",
       "      <td>0.072721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.433700</td>\n",
       "      <td>2.162472</td>\n",
       "      <td>0.004458</td>\n",
       "      <td>0.072636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.383300</td>\n",
       "      <td>2.158943</td>\n",
       "      <td>0.004458</td>\n",
       "      <td>0.072662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=1.4887142494916916, metrics={'train_runtime': 981.8586, 'train_samples_per_second': 8.148, 'train_steps_per_second': 1.018, 'total_flos': 4397852000256000.0, 'train_loss': 1.4887142494916916, 'epoch': 8.0})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9f366702-5c66-415a-be26-ff1754bd2a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:33]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefix-Tuned Model Scores: {'eval_loss': 2.1589431762695312, 'eval_bleu': 0.00445772802493562, 'eval_rougeL': 0.07266155761421292, 'eval_runtime': 36.5133, 'eval_samples_per_second': 1.095, 'eval_steps_per_second': 0.548, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate prefix-tuned model\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Prefix-Tuned Model Scores:\", eval_results)\n",
    "\n",
    "# Log to W&B\n",
    "wandb.log({\n",
    "    \"prefix_bleu\": eval_results[\"eval_bleu\"],\n",
    "    \"prefix_rougeL\": eval_results[\"eval_rougeL\"]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c09543d0-233b-47bf-83bd-8edba17379d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison:\n",
      "Base Model - BLEU: 0.0000, ROUGE-L: 0.1344\n",
      "Prefix-Tuned - BLEU: 0.0045, ROUGE-L: 0.0727\n"
     ]
    }
   ],
   "source": [
    "# Compare with base model\n",
    "print(\"\\nComparison:\")\n",
    "print(f\"Base Model - BLEU: {base_bleu_avg:.4f}, ROUGE-L: {base_rouge_avg:.4f}\")\n",
    "print(f\"Prefix-Tuned - BLEU: {eval_results['eval_bleu']:.4f}, ROUGE-L: {eval_results['eval_rougeL']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8abbe400-34aa-462e-a44a-db8107287b51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f02f7f47343f4808ad1cddaf8e6068e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/rtweera/qwen25-0.5b-prefix-tuning/commit/c1963aa82ab32e2995439b7c1d8e83f0d5157f28', commit_message='qwen25-0.5b-prefix-tuning-question-generation', commit_description='', oid='c1963aa82ab32e2995439b7c1d8e83f0d5157f28', pr_url=None, repo_url=RepoUrl('https://huggingface.co/rtweera/qwen25-0.5b-prefix-tuning', endpoint='https://huggingface.co', repo_type='model', repo_id='rtweera/qwen25-0.5b-prefix-tuning'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push model to Hugging Face Hub\n",
    "trainer.push_to_hub(\"qwen25-0.5b-prefix-tuning-question-generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "204f0942-da73-4799-a407-5bd78933395f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>base_bleu</td><td>▁</td></tr><tr><td>base_rougeL</td><td>▁</td></tr><tr><td>eval/bleu</td><td>▁▂▃▆█████</td></tr><tr><td>eval/loss</td><td>█▅▃▂▂▁▁▁▁</td></tr><tr><td>eval/rougeL</td><td>▁▃█▅▅▆▆▆▆</td></tr><tr><td>eval/runtime</td><td>█▂▂▁▁▁▁▁▁</td></tr><tr><td>eval/samples_per_second</td><td>▁▇▇██████</td></tr><tr><td>eval/steps_per_second</td><td>▁▇▇▇▇████</td></tr><tr><td>prefix_bleu</td><td>▁</td></tr><tr><td>prefix_rougeL</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▇▇▄█▄▄▃▃▂▄▅▃▂▂▂▄▂▂▂▂▃▂▂▂▂▁▁▂▁▁▅▁▁█▁▂▁▂▂▁</td></tr><tr><td>train/learning_rate</td><td>█████▇▇▆▆▆▆▆▆▆▆▅▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▇▆▆▆▆▆▅▄▄▄▄▃▂▄▂▁▃▃▂▃▂▁▂▂▁▂▂▂▁▂▂▂▂▂▃▂▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>base_bleu</td><td>0.0</td></tr><tr><td>base_rougeL</td><td>0.13438</td></tr><tr><td>eval/bleu</td><td>0.00446</td></tr><tr><td>eval/loss</td><td>2.15894</td></tr><tr><td>eval/rougeL</td><td>0.07266</td></tr><tr><td>eval/runtime</td><td>36.5133</td></tr><tr><td>eval/samples_per_second</td><td>1.095</td></tr><tr><td>eval/steps_per_second</td><td>0.548</td></tr><tr><td>prefix_bleu</td><td>0.00446</td></tr><tr><td>prefix_rougeL</td><td>0.07266</td></tr><tr><td>total_flos</td><td>4397852000256000.0</td></tr><tr><td>train/epoch</td><td>8</td></tr><tr><td>train/global_step</td><td>1000</td></tr><tr><td>train/grad_norm</td><td>0.64854</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.3833</td></tr><tr><td>train_loss</td><td>1.48871</td></tr><tr><td>train_runtime</td><td>981.8586</td></tr><tr><td>train_samples_per_second</td><td>8.148</td></tr><tr><td>train_steps_per_second</td><td>1.018</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2025/May/06 - 13:17:39</strong> at: <a href='https://wandb.ai/rtw-rtweera/prefix-tuning-qwen-25-question-generation/runs/ix4rcs7s' target=\"_blank\">https://wandb.ai/rtw-rtweera/prefix-tuning-qwen-25-question-generation/runs/ix4rcs7s</a><br> View project at: <a href='https://wandb.ai/rtw-rtweera/prefix-tuning-qwen-25-question-generation' target=\"_blank\">https://wandb.ai/rtw-rtweera/prefix-tuning-qwen-25-question-generation</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250506_074745-ix4rcs7s/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Finish W&B run\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
