{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11903392,"sourceType":"datasetVersion","datasetId":7482623}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install unsloth wandb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:04:56.773270Z","iopub.execute_input":"2025-05-22T10:04:56.773909Z","iopub.status.idle":"2025-05-22T10:07:57.803862Z","shell.execute_reply.started":"2025-05-22T10:04:56.773883Z","shell.execute_reply":"2025-05-22T10:07:57.803158Z"}},"outputs":[{"name":"stdout","text":"Collecting unsloth\n  Downloading unsloth-2025.5.7-py3-none-any.whl.metadata (47 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.9)\nCollecting unsloth_zoo>=2025.5.8 (from unsloth)\n  Downloading unsloth_zoo-2025.5.8-py3-none-any.whl.metadata (8.0 kB)\nRequirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (2.6.0+cu124)\nCollecting xformers>=0.0.27.post2 (from unsloth)\n  Downloading xformers-0.0.30-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\nCollecting bitsandbytes (from unsloth)\n  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\nRequirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.2.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from unsloth) (25.0)\nCollecting tyro (from unsloth)\n  Downloading tyro-0.9.21-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: transformers!=4.47.0,==4.51.3 in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.51.3)\nRequirement already satisfied: datasets>=3.4.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.6.0)\nRequirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.2.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unsloth) (7.0.0)\nRequirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.45.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unsloth) (1.26.4)\nRequirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (1.5.2)\nCollecting trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9 (from unsloth)\n  Downloading trl-0.15.2-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.14.0)\nRequirement already satisfied: protobuf<4.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.20.3)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.31.1)\nRequirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.1.9)\nRequirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.32.2)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.21.0+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,==4.51.3->unsloth) (3.18.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,==4.51.3->unsloth) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,==4.51.3->unsloth) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,==4.51.3->unsloth) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,==4.51.3->unsloth) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,==4.51.3->unsloth) (0.5.3)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\nRequirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.4)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.25.1)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\nRequirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.2)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth) (0.70.16)\nCollecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->unsloth) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers!=4.47.0,==4.51.3->unsloth) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers!=4.47.0,==4.51.3->unsloth) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers!=4.47.0,==4.51.3->unsloth) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers!=4.47.0,==4.51.3->unsloth) (2025.4.26)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.4.0->unsloth) (1.3.0)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (14.0.0)\nCollecting cut_cross_entropy (from unsloth_zoo>=2025.5.8->unsloth)\n  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.5.8->unsloth) (11.1.0)\nCollecting msgspec (from unsloth_zoo>=2025.5.8->unsloth)\n  Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\nCollecting torch>=2.4.0 (from unsloth)\n  Downloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\nCollecting sympy>=1.13.3 (from torch>=2.4.0->unsloth)\n  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\nCollecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.6.77 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.6.80 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.5.1.17 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12 (from nvidia-cudnn-cu12==9.1.0.70->torch>=2.4.0->unsloth)\n  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.3.0.4 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.7.77 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.7.1.2 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12 (from nvidia-cusolver-cu12==11.6.1.9->torch>=2.4.0->unsloth)\n  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparselt-cu12==0.6.3 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\nCollecting nvidia-nccl-cu12==2.26.2 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\nCollecting nvidia-nvtx-cu12==12.6.77 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.6.1.9->torch>=2.4.0->unsloth)\n  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufile-cu12==1.11.1.6 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting triton>=3.0.0 (from unsloth)\n  Downloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers->unsloth) (8.7.0)\nINFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\nCollecting torchvision (from unsloth)\n  Downloading torchvision-0.22.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\nRequirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (0.16)\nCollecting shtab>=1.5.6 (from tyro->unsloth)\n  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\nRequirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (4.4.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (3.11.18)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (2.19.1)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers->unsloth) (3.21.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->unsloth) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->unsloth) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->unsloth) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->unsloth) (2024.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.4.1->unsloth) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.4.1->unsloth) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.4.1->unsloth) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (1.20.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->unsloth) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (0.1.2)\nDownloading unsloth-2025.5.7-py3-none-any.whl (265 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading trl-0.15.2-py3-none-any.whl (318 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading unsloth_zoo-2025.5.8-py3-none-any.whl (146 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.6/146.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading xformers-0.0.30-cp311-cp311-manylinux_2_28_x86_64.whl (31.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.5/31.5 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl (865.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.2/865.2 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m98.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torchvision-0.22.0-cp311-cp311-manylinux_2_28_x86_64.whl (7.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m105.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tyro-0.9.21-py3-none-any.whl (125 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.3/125.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading shtab-1.7.2-py3-none-any.whl (14 kB)\nDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m98.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\nDownloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (210 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, triton, sympy, shtab, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, msgspec, fsspec, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, tyro, nvidia-cusolver-cu12, torch, cut_cross_entropy, trl, xformers, unsloth_zoo, torchvision, bitsandbytes, unsloth\n  Attempting uninstall: nvidia-cusparselt-cu12\n    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n  Attempting uninstall: triton\n    Found existing installation: triton 3.2.0\n    Uninstalling triton-3.2.0:\n      Successfully uninstalled triton-3.2.0\n  Attempting uninstall: sympy\n    Found existing installation: sympy 1.13.1\n    Uninstalling sympy-1.13.1:\n      Successfully uninstalled sympy-1.13.1\n  Attempting uninstall: nvidia-nvtx-cu12\n    Found existing installation: nvidia-nvtx-cu12 12.4.127\n    Uninstalling nvidia-nvtx-cu12-12.4.127:\n      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.21.5\n    Uninstalling nvidia-nccl-cu12-2.21.5:\n      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\n    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\n    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\n  Attempting uninstall: torch\n    Found existing installation: torch 2.6.0+cu124\n    Uninstalling torch-2.6.0+cu124:\n      Successfully uninstalled torch-2.6.0+cu124\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.21.0+cu124\n    Uninstalling torchvision-0.21.0+cu124:\n      Successfully uninstalled torchvision-0.21.0+cu124\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ntorchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.7.0 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nfastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed bitsandbytes-0.45.5 cut_cross_entropy-25.1.1 fsspec-2025.3.0 msgspec-0.19.0 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 shtab-1.7.2 sympy-1.14.0 torch-2.7.0 torchvision-0.22.0 triton-3.3.0 trl-0.15.2 tyro-0.9.21 unsloth-2025.5.7 unsloth_zoo-2025.5.8 xformers-0.0.30\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Unsloth","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nsecrets_client = UserSecretsClient()\n\nfrom unsloth import FastLanguageModel\nimport torch\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\nqwen_models = [\n    \"unsloth/Qwen2.5-3B-Instruct-unsloth-bnb-4bit\",\n    \"unsloth/Qwen2.5-7B-Instruct-unsloth-bnb-4bit\", # unsloth-bnb-4bit are selectively quantized for more accuracy\n    \"unsloth/Qwen2.5-3B-Instruct-bnb-4bit\", # not selectively quantized\n] # More models at https://huggingface.co/unsloth\n\nMODEL = qwen_models[1]\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = MODEL,\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n    token = secrets_client.get_secret(\"HF_TOKEN\")\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:12:17.017298Z","iopub.execute_input":"2025-05-22T10:12:17.018043Z","iopub.status.idle":"2025-05-22T10:13:07.399591Z","shell.execute_reply.started":"2025-05-22T10:12:17.018019Z","shell.execute_reply":"2025-05-22T10:13:07.398983Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2025.5.7: Fast Qwen2 patching. Transformers: 4.51.3.\n   \\\\   /|    Tesla P100-PCIE-16GB. Num GPUs = 1. Max memory: 15.888 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 6.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.30. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9128d616894543d7a2ab74e9fe79348d"}},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"Add LoRA adaptors","metadata":{}},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    use_rslora = False,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:13:07.400759Z","iopub.execute_input":"2025-05-22T10:13:07.400967Z","iopub.status.idle":"2025-05-22T10:13:14.687521Z","shell.execute_reply.started":"2025-05-22T10:13:07.400952Z","shell.execute_reply":"2025-05-22T10:13:14.686675Z"}},"outputs":[{"name":"stderr","text":"Unsloth 2025.5.7 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## Data prep","metadata":{}},{"cell_type":"code","source":"from unsloth.chat_templates import get_chat_template\n\nSYSTEM_PROMPT = \"\"\"\nYou are Choreo Assistant, an AI trained specifically to help users understand and work with Choreo — the integration platform as a service (iPaaS) developed by WSO2.\n\nYour role is to provide accurate, concise, and actionable answers based strictly on the Choreo documentation you were fine-tuned on. You assist users in performing tasks, understanding features, troubleshooting issues, and navigating workflows within the Choreo platform.\n\nWhen responding:\n- Focus only on Choreo-related topics such as webhooks, services, APIs, integrations, authentication, CI/CD, external consumers, observability, and other Choreo platform capabilities.\n- Avoid speculation or hallucination — if you're unsure or the information is unavailable, say: *\"I'm not certain about that. Please refer to the official Choreo documentation for the latest guidance.\"*\n- Provide answers in a clear, easy-to-follow format, ideally using step-by-step instructions or bullet points where helpful.\n- When relevant, provide links to specific sections of the official documentation.\n\nYou must *not* respond to requests unrelated to Choreo (e.g., jokes, general trivia, non-technical queries).\n\nStay professional, focused, and documentation-aligned at all times.\n\n\"\"\"\n\ndef convert_to_conversations(example, system_prompt=SYSTEM_PROMPT):\n    instruction_part = system_prompt if system_prompt else example['instruction'].strip()\n    user_part = example['input'].strip()\n    assistant_part = example['output'].strip()\n\n    return {\n        \"conversations\": [\n            {\"role\": \"system\", \"content\": instruction_part},\n            {\"role\": \"user\", \"content\": user_part},\n            {\"role\": \"assistant\", \"content\": assistant_part}\n        ]\n    }\n\ndef formatting_prompts_func(examples):\n    convos = examples['conversations']\n    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n    return { \"text\" : texts, }\n\nfrom datasets import load_dataset\ndataset = load_dataset(\"rtweera/choreo-dataset-fixed-leakage\", split = \"train\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:13:14.688469Z","iopub.execute_input":"2025-05-22T10:13:14.688871Z","iopub.status.idle":"2025-05-22T10:13:16.096277Z","shell.execute_reply.started":"2025-05-22T10:13:14.688843Z","shell.execute_reply":"2025-05-22T10:13:16.095581Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/654 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bedeeb8698344f328cc0e9c9243f0b58"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"dataset = dataset.map(convert_to_conversations, remove_columns=[\"instruction\", \"input\", \"output\"], batched=False,) # Non batch operations (like strip()) present","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:13:16.097918Z","iopub.execute_input":"2025-05-22T10:13:16.098167Z","iopub.status.idle":"2025-05-22T10:13:17.950624Z","shell.execute_reply.started":"2025-05-22T10:13:16.098148Z","shell.execute_reply":"2025-05-22T10:13:17.949781Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/654 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"539ddb7e909d415cbd8ef0d14b51947a"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"dataset = dataset.map(formatting_prompts_func, batched=True,) # inside only batch operations are allowed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:13:17.951293Z","iopub.execute_input":"2025-05-22T10:13:17.951517Z","iopub.status.idle":"2025-05-22T10:13:18.166868Z","shell.execute_reply.started":"2025-05-22T10:13:17.951498Z","shell.execute_reply":"2025-05-22T10:13:18.166032Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/654 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4589841f8954ff48dd89d125e81ad9c"}},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"## Run config","metadata":{}},{"cell_type":"code","source":"from datetime import datetime\nimport pytz\n\nMODEL_SAVENAME = MODEL.split('/')[-1]\nnow_utc = datetime.now(pytz.utc)\nnow_colombo = now_utc.astimezone(pytz.timezone('Asia/Colombo'))\ntime_str = now_colombo.strftime('%Y-%b-%d_%H-%M-%S')\nrun_name = f'{time_str}_{MODEL_SAVENAME}_LoRA'\nprint(run_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:13:18.167667Z","iopub.execute_input":"2025-05-22T10:13:18.167925Z","iopub.status.idle":"2025-05-22T10:13:18.195398Z","shell.execute_reply.started":"2025-05-22T10:13:18.167908Z","shell.execute_reply":"2025-05-22T10:13:18.194840Z"}},"outputs":[{"name":"stdout","text":"2025-May-22_15-43-18_Qwen2.5-7B-Instruct-unsloth-bnb-4bit_LoRA\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import wandb\nfrom huggingface_hub import login","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:13:18.196095Z","iopub.execute_input":"2025-05-22T10:13:18.196331Z","iopub.status.idle":"2025-05-22T10:13:18.202721Z","shell.execute_reply.started":"2025-05-22T10:13:18.196311Z","shell.execute_reply":"2025-05-22T10:13:18.202151Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"login(token=secrets_client.get_secret(\"HF_TOKEN\"))\nwandb.login(key=secrets_client.get_secret('WANDB_TOKEN'))\nwandb.init(project=\"choreo-doc-assistant-lora\", name=run_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:13:18.203380Z","iopub.execute_input":"2025-05-22T10:13:18.203580Z","iopub.status.idle":"2025-05-22T10:13:30.948889Z","shell.execute_reply.started":"2025-05-22T10:13:18.203564Z","shell.execute_reply":"2025-05-22T10:13:30.948292Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrtweera\u001b[0m (\u001b[33mrtw-rtweera\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250522_101324-s1pcfqkz</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/rtw-rtweera/choreo-doc-assistant-lora/runs/s1pcfqkz' target=\"_blank\">2025-May-22_15-43-18_Qwen2.5-7B-Instruct-unsloth-bnb-4bit_LoRA</a></strong> to <a href='https://wandb.ai/rtw-rtweera/choreo-doc-assistant-lora' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/rtw-rtweera/choreo-doc-assistant-lora' target=\"_blank\">https://wandb.ai/rtw-rtweera/choreo-doc-assistant-lora</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/rtw-rtweera/choreo-doc-assistant-lora/runs/s1pcfqkz' target=\"_blank\">https://wandb.ai/rtw-rtweera/choreo-doc-assistant-lora/runs/s1pcfqkz</a>"},"metadata":{}},{"execution_count":11,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/rtw-rtweera/choreo-doc-assistant-lora/runs/s1pcfqkz?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7f0f796ba010>"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments, DataCollatorForSeq2Seq\nfrom unsloth import is_bfloat16_supported\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n    dataset_num_proc = 4,\n    packing = False, # Can make training 5x faster for short sequences.\n    args = TrainingArguments(\n        per_device_train_batch_size = 1,\n        gradient_accumulation_steps = 4, # Fixed major bug in latest Unsloth\n        warmup_steps = 5,\n        num_train_epochs = 3, # Set this for 1 full training run.\n        learning_rate = 2e-4,\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        logging_steps = 1,\n        logging_first_step=True,\n        optim = \"paged_adamw_8bit\", # Save more memory\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = run_name,\n        report_to = \"wandb\", # Use this for WandB etc\n        save_steps=20,\n        save_total_limit=4,\n        push_to_hub=True,\n        hub_model_id=run_name\n    ),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:13:30.949622Z","iopub.execute_input":"2025-05-22T10:13:30.949867Z","iopub.status.idle":"2025-05-22T10:13:34.224865Z","shell.execute_reply.started":"2025-05-22T10:13:30.949849Z","shell.execute_reply":"2025-05-22T10:13:34.224018Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=4):   0%|          | 0/654 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87905a37d11c4e9f90c3c840ee01b423"}},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"train_on_responses masks all the user input (as well as system prompt) and only calculate the loss on assistant output\n[https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_Coder_(14B)-Conversational.ipynb#scrollTo=juQiExuBG5Bt]","metadata":{}},{"cell_type":"code","source":"from unsloth.chat_templates import train_on_responses_only\ntrainer = train_on_responses_only(\n    trainer,\n    instruction_part = \"<|im_start|>user\\n\",\n    response_part = \"<|im_start|>assistant\\n\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:13:34.227044Z","iopub.execute_input":"2025-05-22T10:13:34.227272Z","iopub.status.idle":"2025-05-22T10:13:35.029451Z","shell.execute_reply.started":"2025-05-22T10:13:34.227251Z","shell.execute_reply":"2025-05-22T10:13:35.028626Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/654 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdd35cb5e9b54a2d8d3f4040a8e9371c"}},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:13:35.030478Z","iopub.execute_input":"2025-05-22T10:13:35.030768Z","iopub.status.idle":"2025-05-22T12:48:08.995921Z","shell.execute_reply.started":"2025-05-22T10:13:35.030739Z","shell.execute_reply":"2025-05-22T12:48:08.995339Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 654 | Num Epochs = 3 | Total steps = 489\nO^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 4\n\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 4 x 1) = 4\n \"-____-\"     Trainable parameters = 40,370,176/7,000,000,000 (0.58% trained)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Will smartly offload gradients to save VRAM!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='489' max='489' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [489/489 2:34:01, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.399900</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.473300</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.490400</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.515600</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>1.510400</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.215300</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>1.095100</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>1.156900</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>1.155100</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>1.190500</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>1.211200</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>1.159000</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>1.090900</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>1.185100</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>1.002800</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>1.196500</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>1.023000</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>1.050200</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>1.327800</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.025200</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>1.096700</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>1.232200</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>1.151600</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>1.175800</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.944600</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>1.164200</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.996500</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>1.191700</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>1.044500</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.995300</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>1.236200</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>1.123600</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>0.940400</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>1.135000</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>1.277000</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>1.110100</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>0.845800</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>1.144700</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>0.980300</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.161400</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>1.211000</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>1.042300</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>1.091600</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>1.112200</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>1.084400</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>0.970500</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>0.964800</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>0.916700</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>0.934800</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.815700</td>\n    </tr>\n    <tr>\n      <td>51</td>\n      <td>1.104600</td>\n    </tr>\n    <tr>\n      <td>52</td>\n      <td>1.075500</td>\n    </tr>\n    <tr>\n      <td>53</td>\n      <td>1.147100</td>\n    </tr>\n    <tr>\n      <td>54</td>\n      <td>0.986200</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>1.193200</td>\n    </tr>\n    <tr>\n      <td>56</td>\n      <td>0.910000</td>\n    </tr>\n    <tr>\n      <td>57</td>\n      <td>0.928200</td>\n    </tr>\n    <tr>\n      <td>58</td>\n      <td>0.956800</td>\n    </tr>\n    <tr>\n      <td>59</td>\n      <td>0.967000</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.904400</td>\n    </tr>\n    <tr>\n      <td>61</td>\n      <td>0.948800</td>\n    </tr>\n    <tr>\n      <td>62</td>\n      <td>1.051400</td>\n    </tr>\n    <tr>\n      <td>63</td>\n      <td>1.174600</td>\n    </tr>\n    <tr>\n      <td>64</td>\n      <td>0.913900</td>\n    </tr>\n    <tr>\n      <td>65</td>\n      <td>1.086500</td>\n    </tr>\n    <tr>\n      <td>66</td>\n      <td>0.746600</td>\n    </tr>\n    <tr>\n      <td>67</td>\n      <td>1.049500</td>\n    </tr>\n    <tr>\n      <td>68</td>\n      <td>0.905300</td>\n    </tr>\n    <tr>\n      <td>69</td>\n      <td>0.958000</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>1.011200</td>\n    </tr>\n    <tr>\n      <td>71</td>\n      <td>0.939700</td>\n    </tr>\n    <tr>\n      <td>72</td>\n      <td>0.803800</td>\n    </tr>\n    <tr>\n      <td>73</td>\n      <td>1.031900</td>\n    </tr>\n    <tr>\n      <td>74</td>\n      <td>0.941700</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>0.807400</td>\n    </tr>\n    <tr>\n      <td>76</td>\n      <td>0.866900</td>\n    </tr>\n    <tr>\n      <td>77</td>\n      <td>0.790800</td>\n    </tr>\n    <tr>\n      <td>78</td>\n      <td>1.126700</td>\n    </tr>\n    <tr>\n      <td>79</td>\n      <td>1.084900</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.881100</td>\n    </tr>\n    <tr>\n      <td>81</td>\n      <td>0.922500</td>\n    </tr>\n    <tr>\n      <td>82</td>\n      <td>0.779600</td>\n    </tr>\n    <tr>\n      <td>83</td>\n      <td>0.945200</td>\n    </tr>\n    <tr>\n      <td>84</td>\n      <td>0.858900</td>\n    </tr>\n    <tr>\n      <td>85</td>\n      <td>1.033700</td>\n    </tr>\n    <tr>\n      <td>86</td>\n      <td>0.944900</td>\n    </tr>\n    <tr>\n      <td>87</td>\n      <td>0.895700</td>\n    </tr>\n    <tr>\n      <td>88</td>\n      <td>0.941000</td>\n    </tr>\n    <tr>\n      <td>89</td>\n      <td>0.942000</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>1.122900</td>\n    </tr>\n    <tr>\n      <td>91</td>\n      <td>0.943500</td>\n    </tr>\n    <tr>\n      <td>92</td>\n      <td>1.016700</td>\n    </tr>\n    <tr>\n      <td>93</td>\n      <td>0.951600</td>\n    </tr>\n    <tr>\n      <td>94</td>\n      <td>0.822100</td>\n    </tr>\n    <tr>\n      <td>95</td>\n      <td>0.893000</td>\n    </tr>\n    <tr>\n      <td>96</td>\n      <td>0.856400</td>\n    </tr>\n    <tr>\n      <td>97</td>\n      <td>0.911100</td>\n    </tr>\n    <tr>\n      <td>98</td>\n      <td>0.952000</td>\n    </tr>\n    <tr>\n      <td>99</td>\n      <td>0.709000</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.875300</td>\n    </tr>\n    <tr>\n      <td>101</td>\n      <td>0.935500</td>\n    </tr>\n    <tr>\n      <td>102</td>\n      <td>0.990500</td>\n    </tr>\n    <tr>\n      <td>103</td>\n      <td>0.880400</td>\n    </tr>\n    <tr>\n      <td>104</td>\n      <td>0.935200</td>\n    </tr>\n    <tr>\n      <td>105</td>\n      <td>0.872500</td>\n    </tr>\n    <tr>\n      <td>106</td>\n      <td>1.024700</td>\n    </tr>\n    <tr>\n      <td>107</td>\n      <td>0.897300</td>\n    </tr>\n    <tr>\n      <td>108</td>\n      <td>0.929700</td>\n    </tr>\n    <tr>\n      <td>109</td>\n      <td>0.977300</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.940100</td>\n    </tr>\n    <tr>\n      <td>111</td>\n      <td>0.885800</td>\n    </tr>\n    <tr>\n      <td>112</td>\n      <td>0.971800</td>\n    </tr>\n    <tr>\n      <td>113</td>\n      <td>1.005700</td>\n    </tr>\n    <tr>\n      <td>114</td>\n      <td>0.862500</td>\n    </tr>\n    <tr>\n      <td>115</td>\n      <td>1.168600</td>\n    </tr>\n    <tr>\n      <td>116</td>\n      <td>0.668300</td>\n    </tr>\n    <tr>\n      <td>117</td>\n      <td>0.833500</td>\n    </tr>\n    <tr>\n      <td>118</td>\n      <td>0.919700</td>\n    </tr>\n    <tr>\n      <td>119</td>\n      <td>0.938900</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.721300</td>\n    </tr>\n    <tr>\n      <td>121</td>\n      <td>0.882900</td>\n    </tr>\n    <tr>\n      <td>122</td>\n      <td>0.967700</td>\n    </tr>\n    <tr>\n      <td>123</td>\n      <td>1.016100</td>\n    </tr>\n    <tr>\n      <td>124</td>\n      <td>0.892700</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>0.957500</td>\n    </tr>\n    <tr>\n      <td>126</td>\n      <td>0.961200</td>\n    </tr>\n    <tr>\n      <td>127</td>\n      <td>0.878400</td>\n    </tr>\n    <tr>\n      <td>128</td>\n      <td>0.905400</td>\n    </tr>\n    <tr>\n      <td>129</td>\n      <td>0.768500</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.852000</td>\n    </tr>\n    <tr>\n      <td>131</td>\n      <td>0.875000</td>\n    </tr>\n    <tr>\n      <td>132</td>\n      <td>1.036100</td>\n    </tr>\n    <tr>\n      <td>133</td>\n      <td>0.856600</td>\n    </tr>\n    <tr>\n      <td>134</td>\n      <td>0.901300</td>\n    </tr>\n    <tr>\n      <td>135</td>\n      <td>0.802200</td>\n    </tr>\n    <tr>\n      <td>136</td>\n      <td>0.918800</td>\n    </tr>\n    <tr>\n      <td>137</td>\n      <td>0.704900</td>\n    </tr>\n    <tr>\n      <td>138</td>\n      <td>0.780500</td>\n    </tr>\n    <tr>\n      <td>139</td>\n      <td>0.879600</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.775500</td>\n    </tr>\n    <tr>\n      <td>141</td>\n      <td>0.956900</td>\n    </tr>\n    <tr>\n      <td>142</td>\n      <td>1.027600</td>\n    </tr>\n    <tr>\n      <td>143</td>\n      <td>0.934500</td>\n    </tr>\n    <tr>\n      <td>144</td>\n      <td>0.785600</td>\n    </tr>\n    <tr>\n      <td>145</td>\n      <td>0.952800</td>\n    </tr>\n    <tr>\n      <td>146</td>\n      <td>0.914900</td>\n    </tr>\n    <tr>\n      <td>147</td>\n      <td>0.931200</td>\n    </tr>\n    <tr>\n      <td>148</td>\n      <td>0.812100</td>\n    </tr>\n    <tr>\n      <td>149</td>\n      <td>0.753100</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.987700</td>\n    </tr>\n    <tr>\n      <td>151</td>\n      <td>0.794300</td>\n    </tr>\n    <tr>\n      <td>152</td>\n      <td>0.835200</td>\n    </tr>\n    <tr>\n      <td>153</td>\n      <td>1.005300</td>\n    </tr>\n    <tr>\n      <td>154</td>\n      <td>0.843100</td>\n    </tr>\n    <tr>\n      <td>155</td>\n      <td>0.847100</td>\n    </tr>\n    <tr>\n      <td>156</td>\n      <td>0.788100</td>\n    </tr>\n    <tr>\n      <td>157</td>\n      <td>0.894800</td>\n    </tr>\n    <tr>\n      <td>158</td>\n      <td>0.839300</td>\n    </tr>\n    <tr>\n      <td>159</td>\n      <td>0.688400</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.832700</td>\n    </tr>\n    <tr>\n      <td>161</td>\n      <td>0.718900</td>\n    </tr>\n    <tr>\n      <td>162</td>\n      <td>0.848100</td>\n    </tr>\n    <tr>\n      <td>163</td>\n      <td>0.923900</td>\n    </tr>\n    <tr>\n      <td>164</td>\n      <td>1.031600</td>\n    </tr>\n    <tr>\n      <td>165</td>\n      <td>0.687300</td>\n    </tr>\n    <tr>\n      <td>166</td>\n      <td>0.756700</td>\n    </tr>\n    <tr>\n      <td>167</td>\n      <td>0.769100</td>\n    </tr>\n    <tr>\n      <td>168</td>\n      <td>0.808000</td>\n    </tr>\n    <tr>\n      <td>169</td>\n      <td>0.756300</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.704100</td>\n    </tr>\n    <tr>\n      <td>171</td>\n      <td>0.726800</td>\n    </tr>\n    <tr>\n      <td>172</td>\n      <td>0.691000</td>\n    </tr>\n    <tr>\n      <td>173</td>\n      <td>0.649900</td>\n    </tr>\n    <tr>\n      <td>174</td>\n      <td>0.888800</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>0.778600</td>\n    </tr>\n    <tr>\n      <td>176</td>\n      <td>0.816300</td>\n    </tr>\n    <tr>\n      <td>177</td>\n      <td>0.755000</td>\n    </tr>\n    <tr>\n      <td>178</td>\n      <td>0.632700</td>\n    </tr>\n    <tr>\n      <td>179</td>\n      <td>0.505000</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.795400</td>\n    </tr>\n    <tr>\n      <td>181</td>\n      <td>0.702600</td>\n    </tr>\n    <tr>\n      <td>182</td>\n      <td>0.872500</td>\n    </tr>\n    <tr>\n      <td>183</td>\n      <td>0.665400</td>\n    </tr>\n    <tr>\n      <td>184</td>\n      <td>0.647700</td>\n    </tr>\n    <tr>\n      <td>185</td>\n      <td>0.709900</td>\n    </tr>\n    <tr>\n      <td>186</td>\n      <td>0.629500</td>\n    </tr>\n    <tr>\n      <td>187</td>\n      <td>0.603800</td>\n    </tr>\n    <tr>\n      <td>188</td>\n      <td>0.658600</td>\n    </tr>\n    <tr>\n      <td>189</td>\n      <td>0.733800</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.803600</td>\n    </tr>\n    <tr>\n      <td>191</td>\n      <td>0.821600</td>\n    </tr>\n    <tr>\n      <td>192</td>\n      <td>0.850400</td>\n    </tr>\n    <tr>\n      <td>193</td>\n      <td>0.672100</td>\n    </tr>\n    <tr>\n      <td>194</td>\n      <td>0.633000</td>\n    </tr>\n    <tr>\n      <td>195</td>\n      <td>0.712900</td>\n    </tr>\n    <tr>\n      <td>196</td>\n      <td>0.607300</td>\n    </tr>\n    <tr>\n      <td>197</td>\n      <td>0.703600</td>\n    </tr>\n    <tr>\n      <td>198</td>\n      <td>0.547600</td>\n    </tr>\n    <tr>\n      <td>199</td>\n      <td>0.679300</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.580400</td>\n    </tr>\n    <tr>\n      <td>201</td>\n      <td>0.695900</td>\n    </tr>\n    <tr>\n      <td>202</td>\n      <td>0.829400</td>\n    </tr>\n    <tr>\n      <td>203</td>\n      <td>0.641900</td>\n    </tr>\n    <tr>\n      <td>204</td>\n      <td>0.691600</td>\n    </tr>\n    <tr>\n      <td>205</td>\n      <td>0.680200</td>\n    </tr>\n    <tr>\n      <td>206</td>\n      <td>0.590200</td>\n    </tr>\n    <tr>\n      <td>207</td>\n      <td>0.630100</td>\n    </tr>\n    <tr>\n      <td>208</td>\n      <td>0.654400</td>\n    </tr>\n    <tr>\n      <td>209</td>\n      <td>0.692000</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.638700</td>\n    </tr>\n    <tr>\n      <td>211</td>\n      <td>0.663300</td>\n    </tr>\n    <tr>\n      <td>212</td>\n      <td>0.738300</td>\n    </tr>\n    <tr>\n      <td>213</td>\n      <td>0.781500</td>\n    </tr>\n    <tr>\n      <td>214</td>\n      <td>0.738200</td>\n    </tr>\n    <tr>\n      <td>215</td>\n      <td>0.693700</td>\n    </tr>\n    <tr>\n      <td>216</td>\n      <td>0.779100</td>\n    </tr>\n    <tr>\n      <td>217</td>\n      <td>0.663900</td>\n    </tr>\n    <tr>\n      <td>218</td>\n      <td>0.594100</td>\n    </tr>\n    <tr>\n      <td>219</td>\n      <td>0.744000</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.546700</td>\n    </tr>\n    <tr>\n      <td>221</td>\n      <td>0.515500</td>\n    </tr>\n    <tr>\n      <td>222</td>\n      <td>0.617900</td>\n    </tr>\n    <tr>\n      <td>223</td>\n      <td>0.602800</td>\n    </tr>\n    <tr>\n      <td>224</td>\n      <td>0.828600</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>0.709700</td>\n    </tr>\n    <tr>\n      <td>226</td>\n      <td>0.636100</td>\n    </tr>\n    <tr>\n      <td>227</td>\n      <td>0.882800</td>\n    </tr>\n    <tr>\n      <td>228</td>\n      <td>0.633700</td>\n    </tr>\n    <tr>\n      <td>229</td>\n      <td>0.712600</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.838000</td>\n    </tr>\n    <tr>\n      <td>231</td>\n      <td>0.489300</td>\n    </tr>\n    <tr>\n      <td>232</td>\n      <td>0.682000</td>\n    </tr>\n    <tr>\n      <td>233</td>\n      <td>0.726000</td>\n    </tr>\n    <tr>\n      <td>234</td>\n      <td>0.736100</td>\n    </tr>\n    <tr>\n      <td>235</td>\n      <td>0.584800</td>\n    </tr>\n    <tr>\n      <td>236</td>\n      <td>0.486500</td>\n    </tr>\n    <tr>\n      <td>237</td>\n      <td>0.795900</td>\n    </tr>\n    <tr>\n      <td>238</td>\n      <td>0.630100</td>\n    </tr>\n    <tr>\n      <td>239</td>\n      <td>0.669200</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.588400</td>\n    </tr>\n    <tr>\n      <td>241</td>\n      <td>0.771300</td>\n    </tr>\n    <tr>\n      <td>242</td>\n      <td>0.633300</td>\n    </tr>\n    <tr>\n      <td>243</td>\n      <td>0.765800</td>\n    </tr>\n    <tr>\n      <td>244</td>\n      <td>0.591400</td>\n    </tr>\n    <tr>\n      <td>245</td>\n      <td>0.707100</td>\n    </tr>\n    <tr>\n      <td>246</td>\n      <td>0.651300</td>\n    </tr>\n    <tr>\n      <td>247</td>\n      <td>0.669000</td>\n    </tr>\n    <tr>\n      <td>248</td>\n      <td>0.668600</td>\n    </tr>\n    <tr>\n      <td>249</td>\n      <td>0.766700</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.690100</td>\n    </tr>\n    <tr>\n      <td>251</td>\n      <td>0.693500</td>\n    </tr>\n    <tr>\n      <td>252</td>\n      <td>0.619800</td>\n    </tr>\n    <tr>\n      <td>253</td>\n      <td>0.586100</td>\n    </tr>\n    <tr>\n      <td>254</td>\n      <td>0.647600</td>\n    </tr>\n    <tr>\n      <td>255</td>\n      <td>0.831700</td>\n    </tr>\n    <tr>\n      <td>256</td>\n      <td>0.375300</td>\n    </tr>\n    <tr>\n      <td>257</td>\n      <td>0.811100</td>\n    </tr>\n    <tr>\n      <td>258</td>\n      <td>0.586100</td>\n    </tr>\n    <tr>\n      <td>259</td>\n      <td>0.557500</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.574200</td>\n    </tr>\n    <tr>\n      <td>261</td>\n      <td>0.433800</td>\n    </tr>\n    <tr>\n      <td>262</td>\n      <td>0.648700</td>\n    </tr>\n    <tr>\n      <td>263</td>\n      <td>0.642100</td>\n    </tr>\n    <tr>\n      <td>264</td>\n      <td>0.789700</td>\n    </tr>\n    <tr>\n      <td>265</td>\n      <td>0.608900</td>\n    </tr>\n    <tr>\n      <td>266</td>\n      <td>0.629100</td>\n    </tr>\n    <tr>\n      <td>267</td>\n      <td>0.550600</td>\n    </tr>\n    <tr>\n      <td>268</td>\n      <td>0.744800</td>\n    </tr>\n    <tr>\n      <td>269</td>\n      <td>0.574900</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.840100</td>\n    </tr>\n    <tr>\n      <td>271</td>\n      <td>0.509300</td>\n    </tr>\n    <tr>\n      <td>272</td>\n      <td>0.761600</td>\n    </tr>\n    <tr>\n      <td>273</td>\n      <td>0.686200</td>\n    </tr>\n    <tr>\n      <td>274</td>\n      <td>0.546700</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>0.758300</td>\n    </tr>\n    <tr>\n      <td>276</td>\n      <td>0.702700</td>\n    </tr>\n    <tr>\n      <td>277</td>\n      <td>0.700000</td>\n    </tr>\n    <tr>\n      <td>278</td>\n      <td>0.509400</td>\n    </tr>\n    <tr>\n      <td>279</td>\n      <td>0.693300</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.655800</td>\n    </tr>\n    <tr>\n      <td>281</td>\n      <td>0.633900</td>\n    </tr>\n    <tr>\n      <td>282</td>\n      <td>0.655600</td>\n    </tr>\n    <tr>\n      <td>283</td>\n      <td>0.604100</td>\n    </tr>\n    <tr>\n      <td>284</td>\n      <td>0.501000</td>\n    </tr>\n    <tr>\n      <td>285</td>\n      <td>0.700000</td>\n    </tr>\n    <tr>\n      <td>286</td>\n      <td>0.762100</td>\n    </tr>\n    <tr>\n      <td>287</td>\n      <td>0.584600</td>\n    </tr>\n    <tr>\n      <td>288</td>\n      <td>0.804600</td>\n    </tr>\n    <tr>\n      <td>289</td>\n      <td>0.625100</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.639700</td>\n    </tr>\n    <tr>\n      <td>291</td>\n      <td>0.565300</td>\n    </tr>\n    <tr>\n      <td>292</td>\n      <td>0.610200</td>\n    </tr>\n    <tr>\n      <td>293</td>\n      <td>0.592300</td>\n    </tr>\n    <tr>\n      <td>294</td>\n      <td>0.817400</td>\n    </tr>\n    <tr>\n      <td>295</td>\n      <td>0.691200</td>\n    </tr>\n    <tr>\n      <td>296</td>\n      <td>0.526400</td>\n    </tr>\n    <tr>\n      <td>297</td>\n      <td>0.527500</td>\n    </tr>\n    <tr>\n      <td>298</td>\n      <td>0.770000</td>\n    </tr>\n    <tr>\n      <td>299</td>\n      <td>0.621400</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.518900</td>\n    </tr>\n    <tr>\n      <td>301</td>\n      <td>0.594100</td>\n    </tr>\n    <tr>\n      <td>302</td>\n      <td>0.496600</td>\n    </tr>\n    <tr>\n      <td>303</td>\n      <td>0.804400</td>\n    </tr>\n    <tr>\n      <td>304</td>\n      <td>0.655800</td>\n    </tr>\n    <tr>\n      <td>305</td>\n      <td>0.579800</td>\n    </tr>\n    <tr>\n      <td>306</td>\n      <td>0.606800</td>\n    </tr>\n    <tr>\n      <td>307</td>\n      <td>0.717600</td>\n    </tr>\n    <tr>\n      <td>308</td>\n      <td>0.582100</td>\n    </tr>\n    <tr>\n      <td>309</td>\n      <td>0.739400</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.684200</td>\n    </tr>\n    <tr>\n      <td>311</td>\n      <td>0.643900</td>\n    </tr>\n    <tr>\n      <td>312</td>\n      <td>0.475700</td>\n    </tr>\n    <tr>\n      <td>313</td>\n      <td>0.666500</td>\n    </tr>\n    <tr>\n      <td>314</td>\n      <td>0.739900</td>\n    </tr>\n    <tr>\n      <td>315</td>\n      <td>0.704700</td>\n    </tr>\n    <tr>\n      <td>316</td>\n      <td>0.751500</td>\n    </tr>\n    <tr>\n      <td>317</td>\n      <td>0.598700</td>\n    </tr>\n    <tr>\n      <td>318</td>\n      <td>0.986400</td>\n    </tr>\n    <tr>\n      <td>319</td>\n      <td>0.646900</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.523700</td>\n    </tr>\n    <tr>\n      <td>321</td>\n      <td>0.581900</td>\n    </tr>\n    <tr>\n      <td>322</td>\n      <td>0.425800</td>\n    </tr>\n    <tr>\n      <td>323</td>\n      <td>0.617200</td>\n    </tr>\n    <tr>\n      <td>324</td>\n      <td>0.703400</td>\n    </tr>\n    <tr>\n      <td>325</td>\n      <td>0.769100</td>\n    </tr>\n    <tr>\n      <td>326</td>\n      <td>0.576600</td>\n    </tr>\n    <tr>\n      <td>327</td>\n      <td>0.722700</td>\n    </tr>\n    <tr>\n      <td>328</td>\n      <td>0.555600</td>\n    </tr>\n    <tr>\n      <td>329</td>\n      <td>0.417600</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.333200</td>\n    </tr>\n    <tr>\n      <td>331</td>\n      <td>0.558500</td>\n    </tr>\n    <tr>\n      <td>332</td>\n      <td>0.459100</td>\n    </tr>\n    <tr>\n      <td>333</td>\n      <td>0.492100</td>\n    </tr>\n    <tr>\n      <td>334</td>\n      <td>0.432500</td>\n    </tr>\n    <tr>\n      <td>335</td>\n      <td>0.680300</td>\n    </tr>\n    <tr>\n      <td>336</td>\n      <td>0.428900</td>\n    </tr>\n    <tr>\n      <td>337</td>\n      <td>0.556700</td>\n    </tr>\n    <tr>\n      <td>338</td>\n      <td>0.326400</td>\n    </tr>\n    <tr>\n      <td>339</td>\n      <td>0.663900</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.397400</td>\n    </tr>\n    <tr>\n      <td>341</td>\n      <td>0.359300</td>\n    </tr>\n    <tr>\n      <td>342</td>\n      <td>0.442000</td>\n    </tr>\n    <tr>\n      <td>343</td>\n      <td>0.300800</td>\n    </tr>\n    <tr>\n      <td>344</td>\n      <td>0.404700</td>\n    </tr>\n    <tr>\n      <td>345</td>\n      <td>0.550700</td>\n    </tr>\n    <tr>\n      <td>346</td>\n      <td>0.277900</td>\n    </tr>\n    <tr>\n      <td>347</td>\n      <td>0.616100</td>\n    </tr>\n    <tr>\n      <td>348</td>\n      <td>0.510100</td>\n    </tr>\n    <tr>\n      <td>349</td>\n      <td>0.361200</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.478900</td>\n    </tr>\n    <tr>\n      <td>351</td>\n      <td>0.492800</td>\n    </tr>\n    <tr>\n      <td>352</td>\n      <td>0.433400</td>\n    </tr>\n    <tr>\n      <td>353</td>\n      <td>0.455100</td>\n    </tr>\n    <tr>\n      <td>354</td>\n      <td>0.445200</td>\n    </tr>\n    <tr>\n      <td>355</td>\n      <td>0.393600</td>\n    </tr>\n    <tr>\n      <td>356</td>\n      <td>0.496600</td>\n    </tr>\n    <tr>\n      <td>357</td>\n      <td>0.444800</td>\n    </tr>\n    <tr>\n      <td>358</td>\n      <td>0.492900</td>\n    </tr>\n    <tr>\n      <td>359</td>\n      <td>0.449500</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.517500</td>\n    </tr>\n    <tr>\n      <td>361</td>\n      <td>0.497200</td>\n    </tr>\n    <tr>\n      <td>362</td>\n      <td>0.504500</td>\n    </tr>\n    <tr>\n      <td>363</td>\n      <td>0.484300</td>\n    </tr>\n    <tr>\n      <td>364</td>\n      <td>0.402400</td>\n    </tr>\n    <tr>\n      <td>365</td>\n      <td>0.450100</td>\n    </tr>\n    <tr>\n      <td>366</td>\n      <td>0.423200</td>\n    </tr>\n    <tr>\n      <td>367</td>\n      <td>0.333100</td>\n    </tr>\n    <tr>\n      <td>368</td>\n      <td>0.663100</td>\n    </tr>\n    <tr>\n      <td>369</td>\n      <td>0.461200</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.452600</td>\n    </tr>\n    <tr>\n      <td>371</td>\n      <td>0.454800</td>\n    </tr>\n    <tr>\n      <td>372</td>\n      <td>0.699000</td>\n    </tr>\n    <tr>\n      <td>373</td>\n      <td>0.398900</td>\n    </tr>\n    <tr>\n      <td>374</td>\n      <td>0.539200</td>\n    </tr>\n    <tr>\n      <td>375</td>\n      <td>0.446800</td>\n    </tr>\n    <tr>\n      <td>376</td>\n      <td>0.528200</td>\n    </tr>\n    <tr>\n      <td>377</td>\n      <td>0.492700</td>\n    </tr>\n    <tr>\n      <td>378</td>\n      <td>0.408500</td>\n    </tr>\n    <tr>\n      <td>379</td>\n      <td>0.526400</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.348300</td>\n    </tr>\n    <tr>\n      <td>381</td>\n      <td>0.420300</td>\n    </tr>\n    <tr>\n      <td>382</td>\n      <td>0.429700</td>\n    </tr>\n    <tr>\n      <td>383</td>\n      <td>0.509100</td>\n    </tr>\n    <tr>\n      <td>384</td>\n      <td>0.456400</td>\n    </tr>\n    <tr>\n      <td>385</td>\n      <td>0.301600</td>\n    </tr>\n    <tr>\n      <td>386</td>\n      <td>0.443000</td>\n    </tr>\n    <tr>\n      <td>387</td>\n      <td>0.537100</td>\n    </tr>\n    <tr>\n      <td>388</td>\n      <td>0.518200</td>\n    </tr>\n    <tr>\n      <td>389</td>\n      <td>0.525300</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.332200</td>\n    </tr>\n    <tr>\n      <td>391</td>\n      <td>0.496800</td>\n    </tr>\n    <tr>\n      <td>392</td>\n      <td>0.561900</td>\n    </tr>\n    <tr>\n      <td>393</td>\n      <td>0.405400</td>\n    </tr>\n    <tr>\n      <td>394</td>\n      <td>0.363900</td>\n    </tr>\n    <tr>\n      <td>395</td>\n      <td>0.446500</td>\n    </tr>\n    <tr>\n      <td>396</td>\n      <td>0.469000</td>\n    </tr>\n    <tr>\n      <td>397</td>\n      <td>0.313000</td>\n    </tr>\n    <tr>\n      <td>398</td>\n      <td>0.448300</td>\n    </tr>\n    <tr>\n      <td>399</td>\n      <td>0.402400</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.377800</td>\n    </tr>\n    <tr>\n      <td>401</td>\n      <td>0.545400</td>\n    </tr>\n    <tr>\n      <td>402</td>\n      <td>0.395700</td>\n    </tr>\n    <tr>\n      <td>403</td>\n      <td>0.321200</td>\n    </tr>\n    <tr>\n      <td>404</td>\n      <td>0.417300</td>\n    </tr>\n    <tr>\n      <td>405</td>\n      <td>0.432600</td>\n    </tr>\n    <tr>\n      <td>406</td>\n      <td>0.353500</td>\n    </tr>\n    <tr>\n      <td>407</td>\n      <td>0.426500</td>\n    </tr>\n    <tr>\n      <td>408</td>\n      <td>0.468200</td>\n    </tr>\n    <tr>\n      <td>409</td>\n      <td>0.295100</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.299400</td>\n    </tr>\n    <tr>\n      <td>411</td>\n      <td>0.436900</td>\n    </tr>\n    <tr>\n      <td>412</td>\n      <td>0.613700</td>\n    </tr>\n    <tr>\n      <td>413</td>\n      <td>0.322300</td>\n    </tr>\n    <tr>\n      <td>414</td>\n      <td>0.585600</td>\n    </tr>\n    <tr>\n      <td>415</td>\n      <td>0.472300</td>\n    </tr>\n    <tr>\n      <td>416</td>\n      <td>0.323000</td>\n    </tr>\n    <tr>\n      <td>417</td>\n      <td>0.425200</td>\n    </tr>\n    <tr>\n      <td>418</td>\n      <td>0.540100</td>\n    </tr>\n    <tr>\n      <td>419</td>\n      <td>0.486000</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.410100</td>\n    </tr>\n    <tr>\n      <td>421</td>\n      <td>0.379400</td>\n    </tr>\n    <tr>\n      <td>422</td>\n      <td>0.471800</td>\n    </tr>\n    <tr>\n      <td>423</td>\n      <td>0.335000</td>\n    </tr>\n    <tr>\n      <td>424</td>\n      <td>0.544100</td>\n    </tr>\n    <tr>\n      <td>425</td>\n      <td>0.420500</td>\n    </tr>\n    <tr>\n      <td>426</td>\n      <td>0.358500</td>\n    </tr>\n    <tr>\n      <td>427</td>\n      <td>0.511400</td>\n    </tr>\n    <tr>\n      <td>428</td>\n      <td>0.424000</td>\n    </tr>\n    <tr>\n      <td>429</td>\n      <td>0.336300</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.568300</td>\n    </tr>\n    <tr>\n      <td>431</td>\n      <td>0.421500</td>\n    </tr>\n    <tr>\n      <td>432</td>\n      <td>0.351300</td>\n    </tr>\n    <tr>\n      <td>433</td>\n      <td>0.395500</td>\n    </tr>\n    <tr>\n      <td>434</td>\n      <td>0.359500</td>\n    </tr>\n    <tr>\n      <td>435</td>\n      <td>0.385000</td>\n    </tr>\n    <tr>\n      <td>436</td>\n      <td>0.603800</td>\n    </tr>\n    <tr>\n      <td>437</td>\n      <td>0.346100</td>\n    </tr>\n    <tr>\n      <td>438</td>\n      <td>0.435000</td>\n    </tr>\n    <tr>\n      <td>439</td>\n      <td>0.586400</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.609000</td>\n    </tr>\n    <tr>\n      <td>441</td>\n      <td>0.393900</td>\n    </tr>\n    <tr>\n      <td>442</td>\n      <td>0.512800</td>\n    </tr>\n    <tr>\n      <td>443</td>\n      <td>0.349900</td>\n    </tr>\n    <tr>\n      <td>444</td>\n      <td>0.411400</td>\n    </tr>\n    <tr>\n      <td>445</td>\n      <td>0.414700</td>\n    </tr>\n    <tr>\n      <td>446</td>\n      <td>0.450000</td>\n    </tr>\n    <tr>\n      <td>447</td>\n      <td>0.347300</td>\n    </tr>\n    <tr>\n      <td>448</td>\n      <td>0.289500</td>\n    </tr>\n    <tr>\n      <td>449</td>\n      <td>0.614400</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.431300</td>\n    </tr>\n    <tr>\n      <td>451</td>\n      <td>0.481500</td>\n    </tr>\n    <tr>\n      <td>452</td>\n      <td>0.564900</td>\n    </tr>\n    <tr>\n      <td>453</td>\n      <td>0.268500</td>\n    </tr>\n    <tr>\n      <td>454</td>\n      <td>0.375000</td>\n    </tr>\n    <tr>\n      <td>455</td>\n      <td>0.551400</td>\n    </tr>\n    <tr>\n      <td>456</td>\n      <td>0.361600</td>\n    </tr>\n    <tr>\n      <td>457</td>\n      <td>0.578900</td>\n    </tr>\n    <tr>\n      <td>458</td>\n      <td>0.447600</td>\n    </tr>\n    <tr>\n      <td>459</td>\n      <td>0.510700</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.529300</td>\n    </tr>\n    <tr>\n      <td>461</td>\n      <td>0.367100</td>\n    </tr>\n    <tr>\n      <td>462</td>\n      <td>0.435100</td>\n    </tr>\n    <tr>\n      <td>463</td>\n      <td>0.419000</td>\n    </tr>\n    <tr>\n      <td>464</td>\n      <td>0.540900</td>\n    </tr>\n    <tr>\n      <td>465</td>\n      <td>0.479300</td>\n    </tr>\n    <tr>\n      <td>466</td>\n      <td>0.452800</td>\n    </tr>\n    <tr>\n      <td>467</td>\n      <td>0.267300</td>\n    </tr>\n    <tr>\n      <td>468</td>\n      <td>0.386000</td>\n    </tr>\n    <tr>\n      <td>469</td>\n      <td>0.504200</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.471500</td>\n    </tr>\n    <tr>\n      <td>471</td>\n      <td>0.447700</td>\n    </tr>\n    <tr>\n      <td>472</td>\n      <td>0.403400</td>\n    </tr>\n    <tr>\n      <td>473</td>\n      <td>0.481700</td>\n    </tr>\n    <tr>\n      <td>474</td>\n      <td>0.559600</td>\n    </tr>\n    <tr>\n      <td>475</td>\n      <td>0.555100</td>\n    </tr>\n    <tr>\n      <td>476</td>\n      <td>0.399300</td>\n    </tr>\n    <tr>\n      <td>477</td>\n      <td>0.356000</td>\n    </tr>\n    <tr>\n      <td>478</td>\n      <td>0.440800</td>\n    </tr>\n    <tr>\n      <td>479</td>\n      <td>0.447800</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.444100</td>\n    </tr>\n    <tr>\n      <td>481</td>\n      <td>0.530800</td>\n    </tr>\n    <tr>\n      <td>482</td>\n      <td>0.301900</td>\n    </tr>\n    <tr>\n      <td>483</td>\n      <td>0.364600</td>\n    </tr>\n    <tr>\n      <td>484</td>\n      <td>0.363100</td>\n    </tr>\n    <tr>\n      <td>485</td>\n      <td>0.804200</td>\n    </tr>\n    <tr>\n      <td>486</td>\n      <td>0.627800</td>\n    </tr>\n    <tr>\n      <td>487</td>\n      <td>0.335300</td>\n    </tr>\n    <tr>\n      <td>488</td>\n      <td>0.425600</td>\n    </tr>\n    <tr>\n      <td>489</td>\n      <td>0.253200</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":14},{"cell_type":"markdown","source":"## Check generation","metadata":{}},{"cell_type":"code","source":"FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n\nmessages = [\n    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n    {\"role\": \"user\", \"content\": \"How to deploy a webapp in choreo?\"},\n]\ninputs = tokenizer.apply_chat_template(\n    messages,\n    tokenize = True,\n    add_generation_prompt = True, # Must add for generation\n    return_tensors = \"pt\",\n).to(\"cuda\")\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer, skip_prompt = True)\n_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 1024,\n                   use_cache = True, temperature = 1.5, min_p = 0.1) # ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T12:48:53.241529Z","iopub.execute_input":"2025-05-22T12:48:53.242147Z","iopub.status.idle":"2025-05-22T12:49:19.938475Z","shell.execute_reply.started":"2025-05-22T12:48:53.242125Z","shell.execute_reply":"2025-05-22T12:49:19.937912Z"}},"outputs":[{"name":"stderr","text":"The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"To deploy a web application in Choreo, follow these steps:\\n\\n1. **Create a Web Application**:\\n   - Ensure you have created a web application component either from scratch or by starting with a **GitHub** or **Docker Hub** repository.\\n\\n2. **Select Your Component**:\\n   - In the Choreo Console, navigate to the **Component Listing** pane and select the web application for which you want to create a deployment configuration.\\n\\n3. **Open Deployment Settings**:\\n   - Go to the left navigation menu and click on **DevOps**, then select **Deployment Configurations**.\\n\\n4. **Edit Default Deployment Configuration**:\\n   - On the **Deployment Configurations** page, click to edit the default deployment configuration.\\n\\n5. **Define Build Configurations**:\\n   - Fill in the build configurations as needed:\\n     | **Field**           | **Values**                                                                                                                                                                                                                      |\\n     | -------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n     | Dockerfile           | This is the Dockerfile associated with the web application (default: `Dockerfile`).                                                                                                                                            |\\n     | Build Command        | The command used to build your web application source code (default: `npm ci && npm run build`).                                                                                                                               |\\n     | Output Directory     | Directory where the build files will be generated (default: `/build`).                                                                                                                                                    |\\n     | Port                 | The port on which your service will run. For web applications, you can leave this as `5000` (default).                                                                                                                       |\\n     | User Interface Port  | This port should point to where your user interface will run (default: `5000`).                                                                                                                                             |\\n     | Additional Dependencies | Additional Maven or Gradle dependencies required for building the application                                                                                                                                            |\\n\\n6. **Create Deployment Configuration**:\\n   - Once you have filled out all necessary fields, click **Create** to save the new deployment configuration.\\n\\n### Prerequisites\\nBefore you begin the deployment process, ensure you have already created the web application component.\\n\\nFor more detailed steps and additional information, please refer to the official documentation: [Deploy a web application](https://wso2.com/choreo/docs/develop-components/deploy-a-web-application/#step-1-deploy-your-web-application).<|im_end|>\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n\nmessages = [\n    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n    {\"role\": \"user\", \"content\": \"What is choreo?\"},\n]\ninputs = tokenizer.apply_chat_template(\n    messages,\n    tokenize = True,\n    add_generation_prompt = True, # Must add for generation\n    return_tensors = \"pt\",\n).to(\"cuda\")\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer, skip_prompt = True)\n_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 2048,\n                   use_cache = True, temperature = 1.5, min_p = 0.1) # ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T12:49:31.631722Z","iopub.execute_input":"2025-05-22T12:49:31.632233Z","iopub.status.idle":"2025-05-22T12:49:46.125940Z","shell.execute_reply.started":"2025-05-22T12:49:31.632209Z","shell.execute_reply":"2025-05-22T12:49:46.125406Z"}},"outputs":[{"name":"stdout","text":"Choreo is a cloud-native integration platform as a service (iPaaS). It allows developers to easily create, manage, and monitor API proxies and microservices, enabling seamless communication and integration across various applications and services.\\n\\nHere are some key aspects of Choreo:\\n\\n- **Integration Capabilities**: It provides functionalities to connect different systems and services, making it easier to develop comprehensive solutions.\\n- **Service Deployment**: Choreo supports deploying various types of services, including REST APIs, GraphQL APIs, gRPC services, and HTTP servers.\\n- **CI/CD**: It offers integrated CI/CD pipelines, streamlining the development process.\\n- **Observability**: The platform includes robust observability tools to monitor service performance and health, allowing developers to troubleshoot issues effectively.\\n- **User-Friendly Interface**: Choreo is designed to be user-friendly, supporting both command-line interfaces (CLI) and a user-friendly web console for managing services.\\n\\nChoreo enhances agility and productivity in developing integrated cloud-native applications, making it a valuable tool for modern software engineering practices.\\n\\nFor more details, you can check out the [Choreo overview](https://wso2.com/choreo/docs/references/faq/#choreo-overview).<|im_end|>\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"## Generate eval set answers","metadata":{}},{"cell_type":"code","source":"user_prompts = [\n    \"How to deploy a webapp?\",\n    \"How to resolve \\\"\\\"Module not found error\\\"\\\" during the deployment of a Python project?\",\n    \"How to add authentication to my service?\",\n    \"How to add a custom domain to my web app?\",\n    \"How can I test my service?\",\n    \"How can I add environment variables to python webapp?\",\n    \"How to connect the React frontend to the backend?\",\n    \"How can I deploy my backend and then connect it to the frontend deployed in github pages?\",\n    \"How can I implement testing for a full-stack application including automated testing for both frontend and backend components?\",\n    \"I'm getting \\\"\\\"procfile not found\\\"\\\" error for my python service. How do I resolve this?\",\n    \"I'm getting .choreo/endpoints.yaml not found error. How do I resolve this?\",\n    \"Tell me how I can configure a readiness probe?\",\n    \"How to configure Azure as an external IdP?\",\n    \"How can I implement rate limiting on my APIs?\",\n    \"How can i resolve trivy security check errors?\",\n    \"How can I add a config file to react web app?\",\n    \"How can I secure my web app?\",\n    \"What are the steps to secure an API in Choreo?\",\n    \"How can I set up alerts for my service?\",\n    \"How can I add members to my organization\",\n    \"How can I create a new environment for deployment\",\n    \"How to make sure that an api is only visible to admins in my organization?\",\n    \"How to use Choreo CLI locally?\",\n    \"How do I set up automatic scaling for my applications?\",\n    \"What options are available for data storage in Choreo?\",\n    \"How can I create a webhook?\",\n    \"Can I set usage limits for my API based on different tiers\",\n    \"How can I monitor the performance of my services deployed on Choreo?\",\n    \"How can I deploy a new version of my service from a different branch in the same component?\",\n    \"What are the available pricing plans in Choreo?\",\n    \"How can I limit requests coming to my API?\",\n    \"I want to control traffic coming to my API.\",\n    \"My build fails during trivy scan\",\n    \"I get build errors during vulnerability scan stage\",\n    \"I have secret keys as configs for my react web app. How to manage them securely?\",\n    \"What steps can I take to protect my web app?\",\n    \"What methods can I use to protect an API in Choreo?\",\n    \"How do I invite new people to join my organization?\",\n    \"How do I restrict my API so that only admins can see it?\",\n    \"How do I get started with using Choreo CLI on my computer?\",\n    \"How do I configure my applications to scale automatically?\",\n    \"My service gets overloaded with requests how to fix this?\"\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T12:49:51.988046Z","iopub.execute_input":"2025-05-22T12:49:51.988296Z","iopub.status.idle":"2025-05-22T12:49:51.994560Z","shell.execute_reply.started":"2025-05-22T12:49:51.988280Z","shell.execute_reply":"2025-05-22T12:49:51.993904Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"len(user_prompts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T12:49:54.595977Z","iopub.execute_input":"2025-05-22T12:49:54.596488Z","iopub.status.idle":"2025-05-22T12:49:54.602025Z","shell.execute_reply.started":"2025-05-22T12:49:54.596464Z","shell.execute_reply":"2025-05-22T12:49:54.601467Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"42"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"from tqdm import tqdm\noutput_file_path = \"output.txt\"\n\ngen_kwargs = {\n    \"max_new_tokens\": 2048,\n    \"use_cache\": True,\n    \"temperature\": 1.5,\n    \"min_p\": 0.1,\n}\n\n# input_ids = inputs, streamer = text_streamer, max_new_tokens = 2048,\n#                    use_cache = True, temperature = 1.5, min_p = 0.1)\n\nwith open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n    for prompt in tqdm(user_prompts, desc=\"Generating\"):\n        # Create chat format input\n        messages = [\n            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n            {\"role\": \"user\", \"content\": prompt}\n        ]\n        prompt_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\n        # Generate\n        inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(model.device)\n        output_ids = model.generate(**inputs, **gen_kwargs)\n        output_text = tokenizer.decode(output_ids[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n\n        # Write one output per line\n        clean_output = output_text.replace(\"\\n\", \" \").strip()\n        # f.write(output_text.strip().replace(\"\\n\", \" \") + \"\\n\")\n        clean_output = f'\"{clean_output}\"'\n        f.write(clean_output + \"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T12:50:12.785943Z","iopub.execute_input":"2025-05-22T12:50:12.786216Z","iopub.status.idle":"2025-05-22T13:05:25.353956Z","shell.execute_reply.started":"2025-05-22T12:50:12.786195Z","shell.execute_reply":"2025-05-22T13:05:25.353415Z"}},"outputs":[{"name":"stderr","text":"Generating: 100%|██████████| 42/42 [15:12<00:00, 21.73s/it]\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## Load model","metadata":{}},{"cell_type":"code","source":"!pip install unsloth wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T09:57:29.912588Z","iopub.execute_input":"2025-05-22T09:57:29.913272Z","iopub.status.idle":"2025-05-22T10:00:34.784806Z","shell.execute_reply.started":"2025-05-22T09:57:29.913232Z","shell.execute_reply":"2025-05-22T10:00:34.783908Z"}},"outputs":[{"name":"stdout","text":"Collecting unsloth\n  Downloading unsloth-2025.5.7-py3-none-any.whl.metadata (47 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.9)\nCollecting unsloth_zoo>=2025.5.8 (from unsloth)\n  Downloading unsloth_zoo-2025.5.8-py3-none-any.whl.metadata (8.0 kB)\nRequirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (2.6.0+cu124)\nCollecting xformers>=0.0.27.post2 (from unsloth)\n  Downloading xformers-0.0.30-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\nCollecting bitsandbytes (from unsloth)\n  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\nRequirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.2.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from unsloth) (25.0)\nCollecting tyro (from unsloth)\n  Downloading tyro-0.9.21-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: transformers!=4.47.0,==4.51.3 in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.51.3)\nRequirement already satisfied: datasets>=3.4.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.6.0)\nRequirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.2.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unsloth) (7.0.0)\nRequirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.45.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unsloth) (1.26.4)\nRequirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (1.5.2)\nCollecting trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9 (from unsloth)\n  Downloading trl-0.15.2-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.14.0)\nRequirement already satisfied: protobuf<4.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.20.3)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.31.1)\nRequirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.1.9)\nRequirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.32.2)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.21.0+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,==4.51.3->unsloth) (3.18.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,==4.51.3->unsloth) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,==4.51.3->unsloth) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,==4.51.3->unsloth) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,==4.51.3->unsloth) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,==4.51.3->unsloth) (0.5.3)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\nRequirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.4)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.25.1)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\nRequirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.2)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth) (0.70.16)\nCollecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->unsloth) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers!=4.47.0,==4.51.3->unsloth) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers!=4.47.0,==4.51.3->unsloth) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers!=4.47.0,==4.51.3->unsloth) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers!=4.47.0,==4.51.3->unsloth) (2025.4.26)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.4.0->unsloth) (1.3.0)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (14.0.0)\nCollecting cut_cross_entropy (from unsloth_zoo>=2025.5.8->unsloth)\n  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.5.8->unsloth) (11.1.0)\nCollecting msgspec (from unsloth_zoo>=2025.5.8->unsloth)\n  Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\nCollecting torch>=2.4.0 (from unsloth)\n  Downloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\nCollecting sympy>=1.13.3 (from torch>=2.4.0->unsloth)\n  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\nCollecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.6.77 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.6.80 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.5.1.17 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12 (from nvidia-cudnn-cu12==9.1.0.70->torch>=2.4.0->unsloth)\n  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.3.0.4 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.7.77 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.7.1.2 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12 (from nvidia-cusolver-cu12==11.6.1.9->torch>=2.4.0->unsloth)\n  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparselt-cu12==0.6.3 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\nCollecting nvidia-nccl-cu12==2.26.2 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\nCollecting nvidia-nvtx-cu12==12.6.77 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.6.1.9->torch>=2.4.0->unsloth)\n  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufile-cu12==1.11.1.6 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting triton>=3.0.0 (from unsloth)\n  Downloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers->unsloth) (8.7.0)\nINFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\nCollecting torchvision (from unsloth)\n  Downloading torchvision-0.22.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\nRequirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (0.16)\nCollecting shtab>=1.5.6 (from tyro->unsloth)\n  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\nRequirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (4.4.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (3.11.18)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (2.19.1)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers->unsloth) (3.21.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->unsloth) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->unsloth) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->unsloth) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->unsloth) (2024.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.4.1->unsloth) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.4.1->unsloth) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.4.1->unsloth) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (1.20.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->unsloth) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (0.1.2)\nDownloading unsloth-2025.5.7-py3-none-any.whl (265 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading trl-0.15.2-py3-none-any.whl (318 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading unsloth_zoo-2025.5.8-py3-none-any.whl (146 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.6/146.6 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading xformers-0.0.30-cp311-cp311-manylinux_2_28_x86_64.whl (31.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.5/31.5 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl (865.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.2/865.2 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m0:01\u001b[0mm\n\u001b[?25hDownloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torchvision-0.22.0-cp311-cp311-manylinux_2_28_x86_64.whl (7.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m103.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tyro-0.9.21-py3-none-any.whl (125 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.3/125.3 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading shtab-1.7.2-py3-none-any.whl (14 kB)\nDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\nDownloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (210 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, triton, sympy, shtab, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, msgspec, fsspec, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, tyro, nvidia-cusolver-cu12, torch, cut_cross_entropy, trl, xformers, unsloth_zoo, torchvision, bitsandbytes, unsloth\n  Attempting uninstall: nvidia-cusparselt-cu12\n    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n  Attempting uninstall: triton\n    Found existing installation: triton 3.2.0\n    Uninstalling triton-3.2.0:\n      Successfully uninstalled triton-3.2.0\n  Attempting uninstall: sympy\n    Found existing installation: sympy 1.13.1\n    Uninstalling sympy-1.13.1:\n      Successfully uninstalled sympy-1.13.1\n  Attempting uninstall: nvidia-nvtx-cu12\n    Found existing installation: nvidia-nvtx-cu12 12.4.127\n    Uninstalling nvidia-nvtx-cu12-12.4.127:\n      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.21.5\n    Uninstalling nvidia-nccl-cu12-2.21.5:\n      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\n    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\n    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\n  Attempting uninstall: torch\n    Found existing installation: torch 2.6.0+cu124\n    Uninstalling torch-2.6.0+cu124:\n      Successfully uninstalled torch-2.6.0+cu124\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.21.0+cu124\n    Uninstalling torchvision-0.21.0+cu124:\n      Successfully uninstalled torchvision-0.21.0+cu124\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ntorchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.7.0 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nfastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed bitsandbytes-0.45.5 cut_cross_entropy-25.1.1 fsspec-2025.3.0 msgspec-0.19.0 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 shtab-1.7.2 sympy-1.14.0 torch-2.7.0 torchvision-0.22.0 triton-3.3.0 trl-0.15.2 tyro-0.9.21 unsloth-2025.5.7 unsloth_zoo-2025.5.8 xformers-0.0.30\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nsecrets_client = UserSecretsClient()\n\nfrom unsloth import FastLanguageModel\nimport torch\n\nLOAD_NAME = \"rtweera/2025-May-22_12-10-49_Qwen2.5-3B-Instruct-unsloth-bnb-4bit_LoRA\"\n\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\nfrom unsloth import FastLanguageModel\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = LOAD_NAME, # YOUR MODEL YOU USED FOR TRAINING\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n    token = secrets_client.get_secret(\"HF_TOKEN\")\n)\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:00:55.540341Z","iopub.execute_input":"2025-05-22T10:00:55.540743Z","iopub.status.idle":"2025-05-22T10:02:08.968719Z","shell.execute_reply.started":"2025-05-22T10:00:55.540704Z","shell.execute_reply":"2025-05-22T10:02:08.967972Z"}},"outputs":[{"name":"stdout","text":"🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2025-05-22 10:01:06.374252: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747908066.574147      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747908066.631479      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"🦥 Unsloth Zoo will now patch everything to make training faster!\n==((====))==  Unsloth 2025.5.7: Fast Qwen2 patching. Transformers: 4.51.3.\n   \\\\   /|    Tesla P100-PCIE-16GB. Num GPUs = 1. Max memory: 15.888 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 6.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.30. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.36G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a937a8fd3ec34e328f51f6aed7ae769e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/271 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47d7669a563b44dcb50c172bdf40875c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/7.36k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58e38d29f67a4c569f8acc01f8efd4f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ab29f7bae294e9b833c3bc363f345f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"790ecfe8957149e5810ae744444b424d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4417cd17f74442148d55f4dc1da084a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/614 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f93c510abe75420d904808537682045e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"601bee39aa994a79af98601e4ee037e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/120M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d8eb0bcb74e42c6b25f6566a94518fc"}},"metadata":{}},{"name":"stderr","text":"Unsloth 2025.5.7 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): Qwen2ForCausalLM(\n      (model): Qwen2Model(\n        (embed_tokens): Embedding(151936, 2048, padding_idx=151654)\n        (layers): ModuleList(\n          (0-1): 2 x Qwen2DecoderLayer(\n            (self_attn): Qwen2Attention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=256, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=256, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): Qwen2MLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=11008, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=11008, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=11008, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=11008, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=11008, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=11008, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n            (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n          )\n          (2-3): 2 x Qwen2DecoderLayer(\n            (self_attn): Qwen2Attention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=256, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=256, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): Qwen2MLP(\n              (gate_proj): lora.Linear(\n                (base_layer): Linear(in_features=2048, out_features=11008, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=11008, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear(\n                (base_layer): Linear(in_features=2048, out_features=11008, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=11008, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear(\n                (base_layer): Linear(in_features=11008, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=11008, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n            (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n          )\n          (4-29): 26 x Qwen2DecoderLayer(\n            (self_attn): Qwen2Attention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=256, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=256, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): Qwen2MLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=11008, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=11008, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=11008, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=11008, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=11008, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=11008, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n            (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n          )\n          (30): Qwen2DecoderLayer(\n            (self_attn): Qwen2Attention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=256, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=256, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): Qwen2MLP(\n              (gate_proj): lora.Linear(\n                (base_layer): Linear(in_features=2048, out_features=11008, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=11008, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear(\n                (base_layer): Linear(in_features=2048, out_features=11008, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=11008, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear(\n                (base_layer): Linear(in_features=11008, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=11008, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n            (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n          )\n          (31-35): 5 x Qwen2DecoderLayer(\n            (self_attn): Qwen2Attention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=256, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=256, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): Qwen2MLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=11008, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=11008, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=11008, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=11008, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=11008, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=11008, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n            (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n          )\n        )\n        (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n        (rotary_emb): LlamaRotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"SYSTEM_PROMPT = \"\"\"\nYou are Choreo Assistant, an AI trained specifically to help users understand and work with Choreo — the integration platform as a service (iPaaS) developed by WSO2.\n\nYour role is to provide accurate, concise, and actionable answers based strictly on the Choreo documentation you were fine-tuned on. You assist users in performing tasks, understanding features, troubleshooting issues, and navigating workflows within the Choreo platform.\n\nWhen responding:\n- Focus only on Choreo-related topics such as webhooks, services, APIs, integrations, authentication, CI/CD, external consumers, observability, and other Choreo platform capabilities.\n- Avoid speculation or hallucination — if you're unsure or the information is unavailable, say: *\"I'm not certain about that. Please refer to the official Choreo documentation for the latest guidance.\"*\n- Provide answers in a clear, easy-to-follow format, ideally using step-by-step instructions or bullet points where helpful.\n- When relevant, provide links to specific sections of the official documentation.\n\nYou must *not* respond to requests unrelated to Choreo (e.g., jokes, general trivia, non-technical queries).\n\nStay professional, focused, and documentation-aligned at all times.\n\n\"\"\"\n\nmessages = [\n    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n    {\"role\": \"user\", \"content\": \"What is choreo?\"},\n]\ninputs = tokenizer.apply_chat_template(\n    messages,\n    tokenize = True,\n    add_generation_prompt = True, # Must add for generation\n    return_tensors = \"pt\",\n).to(\"cuda\")\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer, skip_prompt = True)\n_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 2048,\n                   use_cache = True, temperature = 1.5, min_p = 0.1) # ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:02:34.916649Z","iopub.execute_input":"2025-05-22T10:02:34.917756Z","iopub.status.idle":"2025-05-22T10:03:06.465016Z","shell.execute_reply.started":"2025-05-22T10:02:34.917722Z","shell.execute_reply":"2025-05-22T10:03:06.464420Z"}},"outputs":[{"name":"stderr","text":"The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"Choreo is a leading integrated cloud-native development platform designed to streamline the process of developing and managing applications across multiple clouds. It simplifies containerized application development, enabling developers to efficiently create, deploy, test, and manage microservices in a single console. Key features of Choreo include:\\n\\n1. **Single Console Development**: Choreo provides a unified platform that facilitates building applications, regardless of their deployment locations, making it easier for teams to collaborate and accelerate their development processes.\\n\\n2. **Full DevOps Workflow**: With Choreo, you can automate tasks like build, deploy, testing, and scaling across various cloud environments, including Azure, AWS, GCP, and Digital Ocean. This includes support for diverse architectures like monolithic, service-oriented, API-first, and full PDP.\\n\\n3. **Containerization Support**: Choreo supports containerization of applications in various languages and frameworks, including Docker and CoreOS Container Linux. This allows for efficient management and scaling of applications.\\n\\n4. **CI/CD Features**: The platform offers robust Continuous Integration and Continuous Deployment functionalities, ensuring that every deployment starts from a known good state, reducing the risk of configuration drift across different environments.\\n\\n5. **Security and Compliance**: Choreo prioritizes security and compliance with industry standards, providing options for key-value, token, and file vaulting mechanisms for secure storage of sensitive data and configurations. Additionally, it ensures that all secrets are securely stored in Kubernetes secrets.\\n\\n6. **Integration Capabilities**: You can build robust applications by integrating components from different cloud providers or environments using a zero-code approach, facilitating seamless integration of services from various suppliers.\\n\\n7. **Observability**: For troubleshooting and monitoring, Choreo provides insights into how components perform across different environments and timeframes, allowing developers to optimize performance and stability effectively.\\n\\nBy leveraging these features, Choreo enables developers to create and maintain complex systems more efficiently while adhering to best practices for software engineering and DevOps.\\n\\nFor more detailed information on Choreo's features and capabilities, you can refer to the [Choreo documentation](https://wso2.com/choreo/docs).<|im_end|>\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}