{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dab997f-9be5-496d-a8c3-53e835c642c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers datasets wandb accelerate deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fafb22c-9347-44f4-90b1-b8fb2f060c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "import wandb\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d232be85-4854-4efe-8472-4f636ab649f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up environment variables for authentication\n",
    "user_secrets = UserSecretsClient()\n",
    "os.environ['HF_TOKEN'] = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "os.environ['WANDB_API_KEY'] = user_secrets.get_secret(\"WANDB_TOKEN\")\n",
    "assert(os.getenv('HF_TOKEN'))\n",
    "assert(os.getenv('WANDB_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2904eadd-98bc-4a50-b122-5c704ffdd12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a unique run name based on timestamp\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "now_utc = datetime.now(pytz.utc)\n",
    "now_colombo = now_utc.astimezone(pytz.timezone('Asia/Colombo'))\n",
    "time_str = now_colombo.strftime('%Y-%b-%d--%H-%M-%S')\n",
    "run_name = f'full-ft-{time_str}'\n",
    "print(run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28e1179-fd67-4fb8-80b5-5345f39a404e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wandb and login to HuggingFace\n",
    "wandb.login(key=os.getenv('WANDB_API_KEY'))\n",
    "wandb.init(project=\"choreo-doc-full-ft\", name=run_name)\n",
    "\n",
    "login(token=os.getenv('HF_TOKEN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62bf517-a329-45a0-b762-c1c9b2b2ec0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "model_id = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,  # Use mixed precision for efficiency\n",
    "    device_map=\"auto\"            # Automatically choose best device setup\n",
    ")\n",
    "\n",
    "# Make sure all parameters are trainable (unlike LoRA which only trains a subset)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecda2a3b-0624-4ccd-ae15-1ea7adae1851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print trainable parameters info\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable parameters: {trainable_params:,} ({100 * trainable_params / total_params:.2f}% of total)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8b4401-00df-42c8-a3d0-9a3730bfa3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"/kaggle/input/choreo-dataset/choreo_dataset.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca489ef-a2ee-4e78-bcf2-b6823ab6fc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation sets\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.1)\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba738a6-af7a-49c1-9b01-affc76843901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess and tokenize dataset\n",
    "def preprocess_function(examples):\n",
    "    # Format conversations using the model's chat template\n",
    "    conversations = []\n",
    "    \n",
    "    for instruction, inp, out in zip(examples['instruction'], examples['input'], examples['output']):\n",
    "        # Create conversation with system instruction, user input, and assistant output\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": instruction},\n",
    "            {\"role\": \"user\", \"content\": inp},\n",
    "            {\"role\": \"assistant\", \"content\": out}\n",
    "        ]\n",
    "        conversations.append(messages)\n",
    "    \n",
    "    # Apply the model's built-in chat template\n",
    "    formatted_texts = [tokenizer.apply_chat_template(conv, tokenize=False, add_generation_prompt=False) \n",
    "                      for conv in conversations]\n",
    "    \n",
    "    # Tokenize the formatted texts\n",
    "    tokenized = tokenizer(\n",
    "        formatted_texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Create labels (for causal LM, typically identical to input_ids)\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    \n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85516db-4b21-4ecc-9115-f8135a36cc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b54f563-abcd-4a97-8bdc-9212c1e88e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to datasets\n",
    "tokenized_train_dataset = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "\n",
    "tokenized_eval_dataset = eval_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=eval_dataset.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146ca0dd-15c6-4063-a71f-c16c9f8ce9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepSpeed configuration for efficient full fine-tuning\n",
    "deepspeed_config = {\n",
    "    \"fp16\": {\n",
    "        \"enabled\": \"auto\",\n",
    "        \"loss_scale\": 0,\n",
    "        \"loss_scale_window\": 1000,\n",
    "        \"initial_scale_power\": 16,\n",
    "        \"hysteresis\": 2,\n",
    "        \"min_loss_scale\": 1\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"AdamW\",\n",
    "        \"params\": {\n",
    "            \"lr\": \"auto\",\n",
    "            \"betas\": \"auto\",\n",
    "            \"eps\": \"auto\",\n",
    "            \"weight_decay\": \"auto\"\n",
    "        }\n",
    "    },\n",
    "    \"scheduler\": {\n",
    "        \"type\": \"WarmupLR\",\n",
    "        \"params\": {\n",
    "            \"warmup_min_lr\": \"auto\",\n",
    "            \"warmup_max_lr\": \"auto\",\n",
    "            \"warmup_num_steps\": \"auto\"\n",
    "        }\n",
    "    },\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 2,\n",
    "        \"offload_optimizer\": {\n",
    "            \"device\": \"cpu\",\n",
    "            \"pin_memory\": True\n",
    "        },\n",
    "        \"allgather_partitions\": True,\n",
    "        \"allgather_bucket_size\": 2e8,\n",
    "        \"overlap_comm\": True,\n",
    "        \"reduce_scatter\": True,\n",
    "        \"reduce_bucket_size\": 2e8,\n",
    "        \"contiguous_gradients\": True\n",
    "    },\n",
    "    \"gradient_accumulation_steps\": \"auto\",\n",
    "    \"gradient_clipping\": \"auto\",\n",
    "    \"steps_per_print\": 10,\n",
    "    \"train_batch_size\": \"auto\",\n",
    "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "    \"wall_clock_breakdown\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ade5203-bd7e-40b2-9344-150bf5f64b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f53c64-f0fc-4f20-a35c-704c9586f589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training arguments - reduced learning rate and batch size for full fine-tuning\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen_choreo_full_ft\",\n",
    "    # For full fine-tuning, we use a lower learning rate than in LoRA\n",
    "    learning_rate=2e-6,\n",
    "    per_device_train_batch_size=2,  # Smaller batch size due to memory constraints with full fine-tuning\n",
    "    gradient_accumulation_steps=8,  # Increased to compensate for smaller batch size\n",
    "    per_device_eval_batch_size=2,\n",
    "    eval_accumulation_steps=8,\n",
    "    num_train_epochs=4,  # Fewer epochs than LoRA, as full fine-tuning converges faster\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    logging_first_step=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    fp16=True if torch.cuda.is_available() else False,\n",
    "    bf16=torch.cuda.is_available() and torch.cuda.is_bf16_supported(),\n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"wandb\",\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=f\"qwen-choreo-full-ft-{time_str}\",  # Custom model ID for HF Hub\n",
    "    hub_strategy=\"end\",  # Push at the end of training\n",
    "    run_name=run_name,\n",
    "    # Additional parameters for full fine-tuning\n",
    "    deepspeed=deepspeed_config,  # Enable DeepSpeed\n",
    "    tf32=True,  # Enable TensorFloat-32 if available\n",
    "    ddp_find_unused_parameters=False,\n",
    "    gradient_checkpointing=True,  # Enable gradient checkpointing for memory efficiency\n",
    "    optim=\"adamw_torch\",\n",
    "    warmup_ratio=0.1,  # Warmup ratio for learning rate scheduler\n",
    "    lr_scheduler_type=\"cosine\",  # Cosine scheduler works well for full fine-tuning\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# %%\n",
    "# Save the model locally\n",
    "model.save_pretrained(\"./qwen_choreo_full_ft_final\")\n",
    "tokenizer.save_pretrained(\"./qwen_choreo_full_ft_final\")\n",
    "\n",
    "# Push the final model to Hub if not already done by the trainer\n",
    "if not training_args.push_to_hub:\n",
    "    model.push_to_hub(f\"qwen-choreo-full-ft-{time_str}\")\n",
    "    tokenizer.push_to_hub(f\"qwen-choreo-full-ft-{time_str}\")\n",
    "\n",
    "# %%\n",
    "# Test the model with a sample\n",
    "test_input = \"How do I configure my applications to scale automatically?\"\n",
    "\n",
    "# Create conversation with the test input\n",
    "test_messages = [\n",
    "    {\"role\": \"user\", \"content\": test_input}\n",
    "]\n",
    "formatted_test = tokenizer.apply_chat_template(test_messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer(formatted_test, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate response\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    max_new_tokens=1000,\n",
    "    temperature=0.5,\n",
    "    top_p=0.9,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "# Decode and print response\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
