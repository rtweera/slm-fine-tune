{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d61ed77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test merged model before GGUF conversion\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# from peft import PeftModel, PeftConfig\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "254877f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\Ravindu\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "LORA_MODEL_PATH = \"rtweera/qwen_choreo_ft_lora\"  \n",
    "BASE_MODEL_PATH = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "MERGED_MODEL_PATH = \"rtweera/qwen_choreo_merged\" \n",
    "\n",
    "login(token=os.getenv('hf_token'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88d25092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing already merged model from Hub...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4d69cda368444268a2faede141781bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/710 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ravindu\\Documents\\My Projects\\slm-fine-tune\\venv-slm-fine-tune\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Ravindu\\.cache\\huggingface\\hub\\models--rtweera--qwen_choreo_merged. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f8a7de3687f40188c239e73e46cb2b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54ab43d3638947cc9e15aeac1142edf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/256 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a52a26bf738491e8294f4303b5be735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b2d673a4ea04f269d3e06c3711046fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b970c2e2c7845af9461f7f4a49e0436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dc06a202d2d4f12b305bc947949a332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c2dbfeecd504ccea41005b4409d8981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76661dbebf29465a908c588ce28c0394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/644 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded already merged model.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Option 1: Test already merged model (if already pushed it)\n",
    "print(\"Testing already merged model from Hub...\")\n",
    "try:\n",
    "    merged_model = AutoModelForCausalLM.from_pretrained(\n",
    "        MERGED_MODEL_PATH,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MERGED_MODEL_PATH)\n",
    "    using_already_merged = True\n",
    "    print(\"Successfully loaded already merged model.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load already merged model: {e}\")\n",
    "    using_already_merged = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fa2fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Option 2: Create merged model fresh if Option 1 failed\n",
    "# if not using_already_merged:\n",
    "#     print(\"Creating fresh merged model...\")\n",
    "#     # Load base model and tokenizer\n",
    "#     base_model = AutoModelForCausalLM.from_pretrained(\n",
    "#         BASE_MODEL_PATH,\n",
    "#         torch_dtype=torch.bfloat16,\n",
    "#         device_map=\"auto\"\n",
    "#     )\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)\n",
    "\n",
    "#     # Load LoRA and merge\n",
    "#     peft_model = PeftModel.from_pretrained(base_model, LORA_MODEL_PATH)\n",
    "#     merged_model = peft_model.merge_and_unload()\n",
    "#     print(\"Successfully created fresh merged model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "211277be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test the model\n",
    "def test_model(prompt, max_tokens=100):\n",
    "    print(f\"\\nTesting with prompt: '{prompt}'\")\n",
    "    formatted_test = f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    inputs = tokenizer(formatted_test, return_tensors=\"pt\").to(merged_model.device)\n",
    "    \n",
    "    print(\"Generating response...\")\n",
    "    outputs = merged_model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        max_new_tokens=max_tokens,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    print(\"\\nModel response:\")\n",
    "    print(response)\n",
    "    \n",
    "    # Check if response contains end token\n",
    "    if \"<|im_end|>\" in response:\n",
    "        print(\"\\n✅ Response properly terminated with <|im_end|> token\")\n",
    "    else:\n",
    "        print(\"\\n❌ Response did NOT terminate with <|im_end|> token\")\n",
    "    \n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3f8d797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing with prompt: 'What is your name?'\n",
      "Generating response...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model response:\n",
      "<|im_start|>user\n",
      "What is your name?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hello! I am not a person, but rather an artificial intelligence designed to answer questions and provide information on various topics. Is there anything specific you would like to know or discuss?<|im_end|>\n",
      "\n",
      "✅ Response properly terminated with <|im_end|> token\n",
      "\n",
      "Testing with prompt: 'How do I configure my applications to scale automatically?'\n",
      "Generating response...\n",
      "\n",
      "Model response:\n",
      "<|im_start|>user\n",
      "How do I configure my applications to scale automatically?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Configuring your applications to scale automatically involves several steps, including setting up your application's infrastructure and implementing appropriate scaling policies. Here’s a step-by-step guide:\n",
      "\n",
      "### 1. **Understand the Basics**\n",
      "   - **Application Architecture**: Understand how your application is designed. Is it single-node or multi-node?\n",
      "   - **Scalability Needs**: Determine what resources (CPU, memory, storage) are required by each component of your application.\n",
      "\n",
      "### 2. **Set Up Your Application Infrastructure\n",
      "\n",
      "✅ Response properly terminated with <|im_end|> token\n",
      "\n",
      "Testing with prompt: 'Tell me a short joke'\n",
      "Generating response...\n",
      "\n",
      "Model response:\n",
      "<|im_start|>user\n",
      "Tell me a short joke<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Why don't scientists trust atoms? Because they make up everything!<|im_end|>\n",
      "\n",
      "✅ Response properly terminated with <|im_end|> token\n",
      "\n",
      "Testing with prompt: 'What are the steps to deploy a container?'\n",
      "Generating response...\n",
      "\n",
      "Model response:\n",
      "<|im_start|>user\n",
      "What are the steps to deploy a container?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Deploying a container involves several key steps, ensuring that your application is set up for production. Here’s a step-by-step guide:\n",
      "\n",
      "### 1. **Prepare Your Development Environment**\n",
      "   - Ensure you have the necessary dependencies installed.\n",
      "   - Set up Docker and Kubernetes (or equivalent) on your development machine.\n",
      "\n",
      "### 2. **Create a New Container Image**\n",
      "   - Choose a suitable image from Docker Hub or create one manually using `docker build`:\n",
      "     ```bash\n",
      "     docker build\n",
      "\n",
      "✅ Response properly terminated with <|im_end|> token\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test with a variety of prompts\n",
    "test_prompts = [\n",
    "    \"What is your name?\",\n",
    "    \"How do I configure my applications to scale automatically?\",\n",
    "    \"Tell me a short joke\",\n",
    "    \"What are the steps to deploy a container?\"\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    test_model(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a071111c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizer special tokens configuration:\n",
      "{'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}\n",
      "\n",
      "Model generation config:\n",
      "GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "\n",
      "EOS Token ID: [151645, 151643]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check tokenizer special tokens to ensure they're properly configured\n",
    "print(\"\\nTokenizer special tokens configuration:\")\n",
    "print(tokenizer.special_tokens_map)\n",
    "\n",
    "# Check generation config\n",
    "print(\"\\nModel generation config:\")\n",
    "print(merged_model.generation_config)\n",
    "\n",
    "# Verify the EOS token is properly set\n",
    "print(\"\\nEOS Token ID:\", merged_model.generation_config.eos_token_id)\n",
    "if merged_model.generation_config.eos_token_id is None:\n",
    "    print(\"❌ EOS token ID is not set! This could cause infinite generation.\")\n",
    "    # Try to fix by setting the EOS token\n",
    "    eos_token_id = tokenizer.encode(\"<|im_end|>\", add_special_tokens=False)[-1]\n",
    "    print(f\"Setting EOS token ID to: {eos_token_id}\")\n",
    "    merged_model.generation_config.eos_token_id = eos_token_id\n",
    "    # Test again with fixed configuration\n",
    "    print(\"\\nTesting again with fixed EOS token:\")\n",
    "    test_model(\"Tell me another short joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3b4db4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-slm-fine-tune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
