{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a9f20c-ab2f-4665-813c-6b18dc8022c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install peft wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16340fb1-74a9-4fc5-886b-f699f66194ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import numpy as np\n",
    "import os\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "import wandb\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c745c3b8-2450-4454-86bc-7711955d93c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_secrets = UserSecretsClient()\n",
    "os.environ['hf_token'] = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "os.environ['WANDB_API_KEY'] = user_secrets.get_secret(\"WANDB_TOKEN\")\n",
    "assert(os.getenv('hf_token'))\n",
    "assert(os.getenv('WANDB_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c97808a-6155-4230-b458-70ce1de0b995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lora-2025-May-16--13-53-57\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "now_utc = datetime.now(pytz.utc)\n",
    "now_colombo = now_utc.astimezone(pytz.timezone('Asia/Colombo'))\n",
    "time_str = now_colombo.strftime('%Y-%b-%d--%H-%M-%S')\n",
    "run_name = f'lora-{time_str}'\n",
    "print(run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8fe1664-a047-4004-ba2d-dcca082bbeda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrtweera\u001b[0m (\u001b[33mrtw-rtweera\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250516_082406-4u0f0b63</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rtw-rtweera/choreo-doc-ast-ft-lora/runs/4u0f0b63' target=\"_blank\">lora-2025-May-16--13-53-57</a></strong> to <a href='https://wandb.ai/rtw-rtweera/choreo-doc-ast-ft-lora' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rtw-rtweera/choreo-doc-ast-ft-lora' target=\"_blank\">https://wandb.ai/rtw-rtweera/choreo-doc-ast-ft-lora</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rtw-rtweera/choreo-doc-ast-ft-lora/runs/4u0f0b63' target=\"_blank\">https://wandb.ai/rtw-rtweera/choreo-doc-ast-ft-lora/runs/4u0f0b63</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.login(key=os.getenv('WANDB_API_KEY'))\n",
    "wandb.init(project=\"choreo-doc-ast-ft-lora\", name=run_name)\n",
    "\n",
    "login(token=os.getenv('hf_token'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8151388-d42d-47ae-9bc4-a688b53b1620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97a11dea6ca1496ea88e2e6d24f2e0b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4d94f58f29e4f678c30735d2e437998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c026166efdd45c8bd387ef0ffed3a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "018dccd047334554b7a929620808c8e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "755a6ab3e20c4eb08050ee8dcd5b058b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/659 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0476a9af24bf4e1792b339737e52ecf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec711d66d4ab4f11b8baa5c121b6043c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load tokenizer and model\n",
    "model_id = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,  # Use mixed precision for efficiency\n",
    "    device_map=\"auto\"            # Automatically choose best device setup (NOTE: remove if causes problems)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc9c1f4-cf04-4478-9801-ce993f8571a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure model for LoRA fine-tuning (Parameter-Efficient Fine-Tuning)\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                          # Rank\n",
    "    lora_alpha=16,                # Alpha parameter for LoRA scaling\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Attention layers to fine-tune\n",
    "    lora_dropout=0.10,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c10a3c8-4b00-4b9e-ac7e-7099a6ee5d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,081,344 || all params: 495,114,112 || trainable%: 0.2184\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()  # Shows percentage of parameters being trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af8c00b2-29a8-4b58-8044-5ca61dc443c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "613d5be1a465493c9e3c5c822189a70d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"/kaggle/input/choreo-dataset/choreo_dataset.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cfcfec-271c-49c1-b019-a639fcc65080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proper formatting for Qwen models' chat template based on Ollama template\n",
    "# def format_chat(instruction, input_text, output):\n",
    "#     # Using Qwen's chat template structure from Ollama\n",
    "#     if instruction:\n",
    "#         # Use instruction as system message\n",
    "#         formatted = f\"<|im_start|>system\\n{instruction}<|im_end|>\\n\"\n",
    "#     else:\n",
    "#         formatted = \"\"\n",
    "    \n",
    "#     # Add user input (if any)\n",
    "#     if input_text:\n",
    "#         formatted += f\"<|im_start|>user\\n{input_text}<|im_end|>\\n\"\n",
    "    \n",
    "#     # Add assistant response\n",
    "#     formatted += f\"<|im_start|>assistant\\n{output}<|im_end|>\"\n",
    "    \n",
    "#     return formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac3ae639-95f0-4059-a904-106c24c5aa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation sets\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.1)\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "def07fdb-b9dc-4ba1-bf21-95791c4a94b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess and tokenize dataset\n",
    "def preprocess_function(examples):\n",
    "    # We are providing same instruction for every message, so tokenize only once (for optimization)\n",
    "    system_instruction = examples['instruction'][0] # select 1st instruction\n",
    "\n",
    "    # format each example\n",
    "    formatted_texts = []\n",
    "    for inp, out in zip(examples['input'], examples['output']):\n",
    "        # Using consistent system message with varied user inputs and assistant outputs\n",
    "        formatted = f\"<|im_start|>system\\n{system_instruction}<|im_end|>\\n<|im_start|>user\\n{inp}<|im_end|>\\n<|im_start|>assistant\\n{out}<|im_end|>\"\n",
    "        formatted_texts.append(formatted)\n",
    "    \n",
    "    tokenized = tokenizer(\n",
    "        formatted_texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Create labels (for causal LM, typically identical to input_ids)\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    \n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25338a72-daab-4158-9d1c-9a3ffb229e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62e315944a764a75875a234a0f346be4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/607 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9f6940e38414b639cb3f48be9d81ad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/68 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply preprocessing to datasets\n",
    "tokenized_train_dataset = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "\n",
    "tokenized_eval_dataset = eval_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=eval_dataset.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c87c99-1bc6-400a-b0a8-2d595964e236",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_70/1140731509.py:27: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen_choreo_ft_lora\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    eval_accumulation_steps=4,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    logging_first_step=True,\n",
    "    eval_on_start=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=20,\n",
    "    save_strategy=\"steps\",\n",
    "    fp16=True if torch.cuda.is_available() else False,\n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=3,\n",
    "    report_to=\"wandb\",\n",
    "    push_to_hub=True,\n",
    "    run_name=run_name\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b28c522a-a8d8-4a29-b83d-113a1eb6884d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='114' max='114' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [114/114 07:53, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>11.058711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>9.615100</td>\n",
       "      <td>9.616743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>7.998200</td>\n",
       "      <td>7.957623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>6.320400</td>\n",
       "      <td>6.294807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>4.811900</td>\n",
       "      <td>4.884067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.144000</td>\n",
       "      <td>3.989768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=114, training_loss=6.505313045100162, metrics={'train_runtime': 485.708, 'train_samples_per_second': 3.749, 'train_steps_per_second': 0.235, 'total_flos': 4016342565126144.0, 'train_loss': 6.505313045100162, 'epoch': 3.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69cd5bc0-b447-4123-9dde-8820249aaede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./qwen_choreo_ft_final/tokenizer_config.json',\n",
       " './qwen_choreo_ft_final/special_tokens_map.json',\n",
       " './qwen_choreo_ft_final/vocab.json',\n",
       " './qwen_choreo_ft_final/merges.txt',\n",
       " './qwen_choreo_ft_final/added_tokens.json',\n",
       " './qwen_choreo_ft_final/tokenizer.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model\n",
    "model.save_pretrained(\"./qwen_choreo_ft_final\")\n",
    "tokenizer.save_pretrained(\"./qwen_choreo_ft_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1319ba04-36e0-4dc7-86f0-8152bc6b0158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "How can I enable rate limiting for aan API in choreo?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "To enable rate limiting for an API using the Choreo framework, you typically need to configure your application to limit the number of requests that it makes per unit time (e.g., per second). This is usually done by setting a rate limit on your backend service.\n",
      "\n",
      "Here are the general steps to achieve this:\n",
      "\n",
      "### 1. Define Rate Limits\n",
      "\n",
      "First, define how many requests you want to make within a given time frame. For example, if you want to allow up to 500 requests per minute, you would set the following rate limits:\n",
      "\n",
      "```python\n",
      "from choreo.api import RateLimitingAPI\n",
      "\n",
      "# Create an instance of the Rate Limiting API\n",
      "rate_limit = RateLimitingAPI()\n",
      "\n",
      "# Set the maximum number of requests allowed per minute\n",
      "rate_limit.set_max_requests_per_minute(500)\n",
      "\n",
      "# Example usage:\n",
      "response = rate_limit.request(\"GET\", \"/api/v1/data\")\n",
      "```\n",
      "\n",
      "### 2. Implement Rate Limiting Logic\n",
      "\n",
      "You also need to implement logic to handle these rate limits. This might involve checking the current request count against the maximum allowed and retrying failed requests based on the rate limit settings.\n",
      "\n",
      "### 3. Monitor and Adjust\n",
      "\n",
      "Regularly monitor the rate limit usage and adjust the rate limit settings as needed. You might consider implementing a logging mechanism or using a tool like `RateLimiter` from the `requests` library to log the number of requests made and the time taken.\n",
      "\n",
      "### 4. Testing\n",
      "\n",
      "Before deploying your application, thoroughly test it with various traffic levels to ensure that it behaves correctly under different conditions.\n",
      "\n",
      "### Example Code\n",
      "\n",
      "Here's an example of how you might use the `RateLimitingAPI` to enforce a rate limit on making GET requests to `/api/v1/data`:\n",
      "\n",
      "```python\n",
      "from choreo.api import RateLimitingAPI\n",
      "\n",
      "# Create an instance of the Rate Limiting API\n",
      "rate_limit = RateLimitingAPI()\n",
      "\n",
      "# Set the maximum number of requests allowed per minute\n",
      "rate_limit.set_max_requests_per_minute(500)\n",
      "\n",
      "# Example usage:\n",
      "def check_rate_limit():\n",
      "    # Make a GET request to /api/v1/data\n",
      "    response = rate_limit.request(\"GET\", \"/api/v1/data\")\n",
      "    \n",
      "    # Check if the request was successful\n",
      "    if response.status_code == 200:\n",
      "        print(f\"Request successful: {response.text}\")\n",
      "    else:\n",
      "        print(f\"Request failed: {response.text}\")\n",
      "\n",
      "# Simulate some traffic\n",
      "check_rate_limit()\n",
      "```\n",
      "\n",
      "In this example, we create a rate-limiting API that sets a maximum of 500 requests per minute. The `check_rate_limit` function simulates a request to `/api/v1/data`, which should succeed since it's not exceeding the rate limit.\n",
      "\n",
      "By following these steps, you can effectively enable rate limiting for your API using the Choreo framework.<|im_end|>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mlora-2025-May-16--13-53-57\u001b[0m at: \u001b[34mhttps://wandb.ai/rtw-rtweera/choreo-doc-ast-ft-lora/runs/4u0f0b63\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250516_082406-4u0f0b63/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Optionally, test the model with a sample\n",
    "test_input = \"How can I enable rate limiting for aan API in choreo?\"\n",
    "formatted_test = f\"<|im_start|>user\\n{test_input}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "inputs = tokenizer(formatted_test, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate response\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    max_new_tokens=1000,\n",
    "    temperature=0.5,\n",
    "    top_p=0.9,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "# Decode and print response\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d6a3e1-e70f-4eea-a98c-222c24a87876",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
